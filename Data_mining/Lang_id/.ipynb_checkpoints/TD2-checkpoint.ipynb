{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline:**\n",
    "1. Read raw data\n",
    "2. Clean data\n",
    "3. Try existing language identification libraries\n",
    "4. Construct our own dictionary-based language classifier\n",
    "5. Construct our own language classifier using supervised learning\n",
    "\n",
    "**What you need to do:** \n",
    "- Read and execute the source code below and answer the questions in **EXERCISE 1 - EXERCISE 6**. \n",
    "- **Submit** the modifiled file ``TD2.ipynb`` on google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set the font size of plots\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_files = ['./langid_data_TUN-AR.txt', './langid_data_ARA.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    print('Reading file ' + filename + \"...\")\n",
    "    with open(filename, \"r\", encoding='utf8') as textfile:\n",
    "        L = []\n",
    "        for line in textfile:\n",
    "            L.append(line.strip())\n",
    "        print('File contains ', len(L), \"lines.\\n\")\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ./langid_data_TUN-AR.txt...\n",
      "File contains  13932 lines.\n",
      "\n",
      "Reading file ./langid_data_ARA.txt...\n",
      "File contains  21787 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tun_corpus = read_text_file(corpus_files[0])\n",
    "ara_corpus = read_text_file(corpus_files[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "For language identification, we usually only need to remove non-word characters and replace them with spaces. But since our corpus contains social media text, a few other operations are necessary.\n",
    "\n",
    "- Remove non-word symbols (punctuation, math symbols, emoticons, URLs, hashtags, etc.).\n",
    "- Replace punctuation and white space with a single space.\n",
    "- Normalize word elongations and word repetitions.\n",
    "- Remove documents that contain a large fraction of latin characters (some documents contain english or french words).\n",
    "- Remove very short documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This just a simple js jshd jhsd js dh asjh test'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# regexp for word elongation: matches 3 or more repetitions of a word character.\n",
    "two_plus_letters_RE = re.compile(r\"(\\w)\\1{1,}\", re.DOTALL)\n",
    "three_plus_letters_RE = re.compile(r\"(\\w)\\1{2,}\", re.DOTALL)\n",
    "# regexp for repeated words\n",
    "two_plus_words_RE = re.compile(r\"(\\w+\\s+)\\1{1,}\", re.DOTALL)\n",
    "\n",
    "\n",
    "def cleanup_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "\n",
    "    # Remove user mentions of the form @username\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "    \n",
    "    # Replace special html-encoded characters with their ASCII equivalent, for example: &#39 ==> '\n",
    "    #if re.search(\"&#\",text):\n",
    "        #text = html.unescape(text)\n",
    "\n",
    "    # Remove special useless characters such as _x000D_\n",
    "    text = re.sub(r'_[xX]000[dD]_', '', text)\n",
    "\n",
    "    # Replace all non-word characters (such as emoticons, punctuation, end of line characters, etc.) with a space\n",
    "    text = re.sub('[\\W_]', ' ', text)\n",
    "\n",
    "    # Remove redundant white spaces\n",
    "    text = text.strip()\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "\n",
    "    # normalize word elongations (characters repeated more than twice)\n",
    "    text = two_plus_letters_RE.sub(r\"\\1\\1\", text)\n",
    "\n",
    "    # remove repeated words\n",
    "    text = two_plus_words_RE.sub(r\"\\1\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# unit test of this function\n",
    "cleanup_text(\"This is just a simple. js .... jshd)jhsd__js--dh \\n\\n asjh\\n test !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 1\n",
    "\n",
    "Do the following operations for **each** corpus. Store the new corpora in new variables called ``tun_corpus_clean`` and ``ara_corpus_clean``\n",
    "1. Clean up each document in the corpus using the ``cleanup_text`` function given above.\n",
    "2. Remove all documents that contain a large fraction of latin characters (for example more than 70%).  **Hint**: use a regular expression with pattern [a-zA-Z]\n",
    "3. Remove very short documents (containing less than 10 characters, for e.g.).\n",
    "4. Display the number of documents in the clean corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13932 21787\n"
     ]
    }
   ],
   "source": [
    "## COMPLETE THE CODE BELOW\n",
    "#1\n",
    "tun_corpus_clean = [cleanup_text(doc) for doc in tun_corpus]\n",
    "ara_corpus_clean = [cleanup_text(doc) for doc in ara_corpus]\n",
    "\n",
    "print(len(tun_corpus_clean), len(ara_corpus_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "#tun_corpus_clean=[re.sub('[a-zA-Z]+','',doc) for doc in tun_corpus_clean]\n",
    "tun_corpus_clean2=[doc for doc in tun_corpus_clean if (len(re.findall('[a-zA-Z]+',doc))/len(doc))<0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "#ara_corpus_clean=[re.sub(' [a-zA-Z]+','',doc) for doc in ara_corpus_clean]\n",
    "ara_corpus_clean2=[doc for doc in ara_corpus_clean if (len(re.findall('[a-zA-Z]+',doc))/len(doc))<0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12039 21459\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "tun_corpus_clean3 = [cleanup_text(doc) for doc in tun_corpus_clean2 if len(doc)>10]\n",
    "ara_corpus_clean3 = [cleanup_text(doc) for doc in ara_corpus_clean2 if len(doc)>10]\n",
    "print(len(tun_corpus_clean3), len(ara_corpus_clean3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try existing language identification libraries\n",
    "- In TD1, we used NLTK's textcat library.\n",
    "- In this TD, we will use another 2 libraries (``langdetect`` and ``langid``) which are actually more accurate than ``textcat``. However, as you will see they "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load special libraries for language identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = \"This is just a simple test !!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[en:0.9999975192815257]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# langdetect library\n",
    "\n",
    "import langdetect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "try:\n",
    "    res = langdetect.detect_langs(test_doc)   # LANGDETECT\n",
    "    #res = langdetect.detect(test_doc) \n",
    "except LangDetectException:\n",
    "    res = langdetect.language.Language(\"UNK\",0)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('en', 0.9999998819046767)\n",
      "[('en', 0.9999998819046767), ('br', 1.1536448337940777e-07), ('la', 1.4557736338637932e-09)]\n"
     ]
    }
   ],
   "source": [
    "# langid library\n",
    "\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "li = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "print(li.classify(test_doc))\n",
    "print(li.rank(test_doc)[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eng'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK textcat library - JUST IN CASE\n",
    "from nltk.classify.textcat import TextCat\n",
    "\n",
    "# create class instance\n",
    "tc = TextCat()\n",
    "tc.guess_language(test_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ``langdetect`` on a small sample of Tunisian Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a small random sample of 5000 documents from the corpus\n",
    "n = 5000\n",
    "random_indices = np.random.choice(np.arange(len(tun_corpus_clean3)), n, replace=False)\n",
    "small_corpus = [tun_corpus_clean[i] for i in random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_langdetect = []\n",
    "\n",
    "for doc in small_corpus:\n",
    "    try:\n",
    "        res_langdetect.append(langdetect.detect_langs(doc))\n",
    "    except LangDetectException:\n",
    "        res_langdetect.append([langdetect.language.Language(\"UNK\",0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_corpus),len(res_langdetect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>language</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>في كلمة واحدة ابله</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اعلن الحرب و اعمل مجهودك che guivara الفن الشع...</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ملا مجانين وسامي معاهم</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>المرا قالت كلام والراجل قال كلام معناه واحد يك...</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>تلفيق و قيد علي الارهاب هه</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document language  probability\n",
       "0                                 في كلمة واحدة ابله       ar     0.999998\n",
       "1  اعلن الحرب و اعمل مجهودك che guivara الفن الشع...       ar     0.999996\n",
       "2                             ملا مجانين وسامي معاهم       ar     0.999997\n",
       "3  المرا قالت كلام والراجل قال كلام معناه واحد يك...       ar     0.999999\n",
       "4                         تلفيق و قيد علي الارهاب هه       ar     0.999995"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's put the results in a data frame for ease of manipulation\n",
    "\n",
    "def foo1(x):\n",
    "    u = str(x).split(':')\n",
    "    return [u[0],float(u[1])]\n",
    "L = [[small_corpus[i]]+foo1(x[0]) for i,x in enumerate(res_langdetect)]\n",
    "df = pd.DataFrame(L)\n",
    "df.columns = ['document', 'language','probability']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 3\n",
    "1. Plot the distribution of languages (call value_counts() on the ``language`` column of ``df``)\n",
    "2. Plot the histogram of probabilities (call plot(kind='hist') on the ``probability`` column of ``df``)\n",
    "3. What are the top 2 languages identified in this corpus?\n",
    "4. For what fraction of documents is this language identifier more than 80% confident in its decision? (i.e. probability > 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENTER YOUR ANSWERS BELOW\n",
    "\n",
    "...\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e232c50>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ENTER YOUR SOURCE CODE BELOW\n",
    "\n",
    "#1\n",
    "plt.plot(df.language.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c6f5240>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2\n",
    "df.probability.plot(kind='hist')\n",
    "#3\n",
    "# the top 2 languages identified in this corpus are Arabe and farsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9758"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4\n",
    "fraction=len([a for a in df.probability if a >0.8]) / len(df.probability)\n",
    "fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 4 (homework - skip this during class)\n",
    "\n",
    "1. Test both libraries (``langdetect`` and ``langid``) on the **entire** ``ara_corpus_clean`` corpus.\n",
    "2. Which library seems to be more accurate for the Arabic language? Justify based on visual inspection of the results.\n",
    "3. Design a new language identification method that combines both libraries -- basically just use the result corresponding to the higher probability.\n",
    "4. Is this new method more accurate than the individual methods? Again, justify your answer based on visual inspection of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responses:\n",
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a small random sample of 5000 documents from the corpus\n",
    "n = 5000\n",
    "random_indices = np.random.choice(np.arange(len(ara_corpus_clean3)), n, replace=False)\n",
    "small_corpus_ara = [ara_corpus_clean[i] for i in random_indices]\n",
    "\n",
    "#langid\n",
    "res_langid_ara = []\n",
    "for doc in small_corpus_ara:\n",
    "    res_langid_ara.append(li.classify(doc))\n",
    "    \n",
    "#langdetect\n",
    "res_langdetect_ara = []\n",
    "\n",
    "for doc in small_corpus_ara:\n",
    "    try:\n",
    "        res_langdetect_ara.append(langdetect.detect_langs(doc))\n",
    "    except LangDetectException:\n",
    "        res_langdetect_ara.append([langdetect.language.Language(\"UNK\",0)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. According to the following results,Langdetect seems to be more accurate for the Arabic language because it has the highest mean. (0.99>0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.996284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.035265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.428568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       probability\n",
       "count  5000.000000\n",
       "mean      0.996284\n",
       "std       0.035265\n",
       "min       0.428568\n",
       "25%       0.999997\n",
       "50%       0.999998\n",
       "75%       0.999999\n",
       "max       1.000000"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L=[foo1(x[0]) for i,x in enumerate(res_langdetect_ara)]\n",
    "df = pd.DataFrame(L)\n",
    "df.columns = ['language','probability']\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.989339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.062126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.169462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       probability\n",
       "count  5000.000000\n",
       "mean      0.989339\n",
       "std       0.062126\n",
       "min       0.169462\n",
       "25%       1.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfid = pd.DataFrame(res_langid_ara)\n",
    "dfid.columns = ['language', 'probability']\n",
    "dfid.head()\n",
    "dfid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The new language identification method that combines both libraries is : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999996747094998],\n",
       " ['ar', 0.9999996905784754],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999990761942698],\n",
       " ['ar', 0.8571407846222152],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999936280556885],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999964832754482],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999997269],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999995714451863],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ur', 0.9999980502712598],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999985376672762],\n",
       " ['ar', 0.9999962982957553],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999860447],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999993],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999984076405863],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999968366042],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999905],\n",
       " ['fa', 0.9999952815884379],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999997341143346],\n",
       " ['ar', 0.9999999999999973],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999997320359453],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999973598584134],\n",
       " ['ar', 0.9999999864704312],\n",
       " ['ar', 0.9999977162081665],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999998649927062],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999852288635],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999803977],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999993006929],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['en', 0.9999999999902391],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999996],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999964842553679],\n",
       " ['ar', 0.9999940368015791],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999990022770002],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999974591],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999991365],\n",
       " ['ar', 0.7142837038932335],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999977317168289],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999975465059914],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999098161682],\n",
       " ['ar', 0.9999999107288653],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999987225761],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999953701],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999964316165342],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999998505601703],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999991],\n",
       " ['ar', 0.9999967489596324],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999994875652095],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999944061742],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999956721440086],\n",
       " ['ar', 0.9999987184709548],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999980373775],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999997509029176],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.882776217122124],\n",
       " ['ar', 0.9999997781902993],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999991007024778],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999462941],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999967],\n",
       " ['en', 1.0],\n",
       " ['ar', 0.9999955075026421],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999997476955649],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999939432995],\n",
       " ['ar', 0.9999995850115468],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999998585947],\n",
       " ['ar', 0.9999999999999949],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999298],\n",
       " ['ar', 0.9999999886810255],\n",
       " ['ar', 0.9999963300070872],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999991491674309],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999976263565473],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999985704153],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999966607849],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9999959889759248],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999998570291],\n",
       " ['ar', 0.9999999850294718],\n",
       " ['ar', 0.9999963284378643],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999968583],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999982],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999964398727567],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999983967217753],\n",
       " ['ar', 0.9999999959026791],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999976488],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999687],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999978058929517],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999797924],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999995182825807],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999976226643],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999883428786],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999977147110876],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999671962452],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999985090031],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999997030378],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999975631848536],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999973912345979],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999949],\n",
       " ['ar', 0.9999995657984274],\n",
       " ['ar', 0.9999999946919047],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999976221],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999996677432144],\n",
       " ['ar', 0.9999999999981315],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999998253],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999968031035648],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999992931491477],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999971],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999958873635217],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999989098942117],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['sl', 0.4285683478819204],\n",
       " ['ar', 0.9999999999977969],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999998681559],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999126277],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999967009323496],\n",
       " ['ar', 0.9999999999997939],\n",
       " ['ar', 0.999999993825516],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999952606102],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999964605351608],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9813245616723961],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999998782882],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999987],\n",
       " ['ar', 0.9999999996610565],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999976492934992],\n",
       " ['ar', 0.9999999876682067],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999963978893613],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999989022765596],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999995203843],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999992324],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999956425111752],\n",
       " ['ar', 0.9999999999609577],\n",
       " ['ar', 0.9999974319827976],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['af', 0.9999958601506705],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ur', 0.9999934239715197],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999161],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999996523],\n",
       " ['ar', 0.9999992881993398],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999978989927141],\n",
       " ['ar', 0.9999975809537305],\n",
       " ['ar', 0.9997817797602139],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999992326715278],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999927],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999659056416],\n",
       " ['ar', 0.9999999999999987],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999993],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999980186101228],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999045213],\n",
       " ['ar', 0.9999999745563763],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999867],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999855138649],\n",
       " ['ar', 0.9999999931932493],\n",
       " ['ar', 0.9999977839934462],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999990296559778],\n",
       " ['ar', 0.9999998922318899],\n",
       " ['ar', 0.999999999999746],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999960124508147],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 0.9999996889215079],\n",
       " ['ar', 0.999998626349254],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999996],\n",
       " ['ar', 0.9999999986876338],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999999999998],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999985110408793],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999998299893529],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999958360983643],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999503],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999996652575679],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999990998],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999975284488336],\n",
       " ['ur', 0.9999964278545374],\n",
       " ['ar', 0.9999999999414564],\n",
       " ['ar', 0.999995936946906],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999998105007],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999970162084487],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999984707034493],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999995866430487],\n",
       " ['ar', 0.9999999965957584],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999982929691205],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999943583],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999959977592],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999989364984113],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999950502817392],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999968899246559],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999276],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999993],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999994437977007],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999996835652555],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999994114201711],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ur', 0.9989963221676086],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999990134442406],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999965806057421],\n",
       " ['ar', 0.9999994723871652],\n",
       " ['ar', 0.9999999999988618],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999329],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999851],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999885],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9900670219823537],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999987166399],\n",
       " ['ar', 0.9999999999982747],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 0.9001655287203467],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999973],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999973813874121],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999845917493],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999964484945],\n",
       " ['ar', 0.9999999168095144],\n",
       " ['ar', 0.9999982537428616],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999983634307419],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999929399],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999965916902201],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999997967911489],\n",
       " ['ar', 0.9999969874557395],\n",
       " ['ar', 0.9999995657733695],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999998721],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999531218986],\n",
       " ['ar', 0.9999999998394358],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999893449384],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['en', 0.9999999999999998],\n",
       " ['ar', 0.999999999954067],\n",
       " ['ar', 0.9999975392784737],\n",
       " ['ar', 0.9999963730102435],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999968639829264],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999995028951463],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999796],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9901169370673935],\n",
       " ['ar', 0.999999999999827],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.8571375739924592],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999956375659818],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999554],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999992630288475],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999982163910892],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999999233931],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999996],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999971478857818],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999962024731677],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999958916021878],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999998421401817],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999961170550569],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999910594],\n",
       " ['en', 1.0],\n",
       " ['ar', 0.9999999390245612],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999996924595642],\n",
       " ['ar', 0.9999999931363364],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999954836428155],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999965455324709],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999947813234],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999981967998323],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999953408],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999998639924225],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999994571799593],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999945589869523],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999975805278639],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999981905314473],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999965179837529],\n",
       " ['ar', 0.9999985220511798],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999998958936707],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999642747218],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999993562549],\n",
       " ['ar', 0.9999981683221217],\n",
       " ['ar', 0.9999973433633792],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999997756906818],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999978833439216],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999207],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999991379527],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999979358409],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ur', 0.9999956698533462],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999911],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 0.9999999998869569],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999988433543],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ...]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine=[]\n",
    "for i in range(0, len(res_langdetect_ara)):\n",
    "    if(df.probability[i]>df2.probability[i]):\n",
    "        combine.append(df.iloc[i].tolist())\n",
    "    \n",
    "    else:\n",
    "        combine.append(df2.iloc[i].tolist())\n",
    "combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.998826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.017869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.428568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       probability\n",
       "count  5000.000000\n",
       "mean      0.998826\n",
       "std       0.017869\n",
       "min       0.428568\n",
       "25%       1.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df = pd.DataFrame(combine)\n",
    "combine_df.columns = ['language','probability']\n",
    "combine_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Yes, this new method is more accurate than the individual methods because it has the highest mean (0.998>0.996>0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a dictionary-based language classifier\n",
    "- **Step 1**: Divide each corpus into a training corpus (70%) and test corpus (30%).\n",
    "- **Step 2**: learn a set of typical words (also called stop words) of **every language** (TUN and ARA) based on its training corpus.\n",
    "- **Step 3**: create a language identification algorithm that takes the list of typical words of each language and a new document as input; and returns the language of this document as output.\n",
    "- **Step 4**: Evaluate the performance of this algorithm based on the test corpus -- calculate classification accuracy, precision, recall, F1, and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 5\n",
    "Implement each step by following the instructions in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1   COMPLETE THE CODE BELOW\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#?train_test_split\n",
    "\n",
    "tun_corpus_clean_train, tun_corpus_clean_test = train_test_split(tun_corpus_clean3, test_size=0.3 )\n",
    "\n",
    "ara_corpus_clean_train, ara_corpus_clean_test = train_test_split(ara_corpus_clean3, test_size=0.3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>في</td>\n",
       "      <td>2.485504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>من</td>\n",
       "      <td>2.887829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>يا</td>\n",
       "      <td>2.989192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>علي</td>\n",
       "      <td>3.058309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>الله</td>\n",
       "      <td>3.269673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word       IDF\n",
       "156    في  2.485504\n",
       "210    من  2.887829\n",
       "250    يا  2.989192\n",
       "138   علي  3.058309\n",
       "34   الله  3.269673"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2   Follow the instructions below\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "P = 1000   ## configuration hyperparameter; you can modify it if you want; see instructions below.\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "\n",
    "## Find typical words of the TUN language\n",
    "\n",
    "# create TfidfVectorizer instance with maxdf = 1.0 so that the most frequent words of the corpus are NOT thrown away\n",
    "bow_model_tun = TfidfVectorizer (max_df = 1.0, min_df = 0.005)\n",
    "\n",
    "# call fit() method with our TUN corpus; this will create the vocabulary of the corpus ...\n",
    "bow_model_tun.fit( tun_corpus_clean_train )\n",
    "\n",
    "# select P words from this vocabulary that have the SMALLEST IDF values -- See the source code in TD1 for help ...\n",
    "tun_vocab=bow_model_tun.get_feature_names ()\n",
    "u=pd.DataFrame(dict(Word=tun_vocab,IDF=bow_model_tun.idf_)).sort_values(\"IDF\", inplace=False, ascending = True)\n",
    "\n",
    "# Note: we do not need to create the DTM matrix in this part.\n",
    "\n",
    "typical_words_tun = u.Word[0:P]\n",
    "\n",
    "u.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>من</td>\n",
       "      <td>1.475638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>في</td>\n",
       "      <td>1.639620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>على</td>\n",
       "      <td>1.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>جدا</td>\n",
       "      <td>2.046158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>الفندق</td>\n",
       "      <td>2.122119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word       IDF\n",
       "1252      من  1.475638\n",
       "994       في  1.639620\n",
       "935      على  1.963800\n",
       "686      جدا  2.046158\n",
       "381   الفندق  2.122119"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_model_ara = TfidfVectorizer (max_df = 1.0, min_df = 0.005)\n",
    "\n",
    "# call fit() method with our TUN corpus; this will create the vocabulary of the corpus ...\n",
    "bow_model_ara.fit( ara_corpus_clean_train )\n",
    "\n",
    "# select P words from this vocabulary that have the SMALLEST IDF values -- See the source code in TD1 for help ...\n",
    "ara_vocab=bow_model_ara.get_feature_names ()\n",
    "v=pd.DataFrame(dict(Word=ara_vocab,IDF=bow_model_ara.idf_)).sort_values(\"IDF\", inplace=False, ascending = True)\n",
    "\n",
    "# Note: we do not need to create the DTM matrix in this part.\n",
    "\n",
    "typical_words_ara = v.Word[0:P]\n",
    "\n",
    "v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TUN\n"
     ]
    }
   ],
   "source": [
    "# Step 3 -- write the algorithm for dictionary-based language identification. \n",
    "#    This algorithm selects the language that has the highest number of typical words in the input document.\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "def dict_langid(typical_words,doc):\n",
    "    sum=0\n",
    "    for i in typical_words:\n",
    "         if i in doc:\n",
    "                sum=sum+1\n",
    "    lang=float(sum/len(doc))\n",
    "    return lang\n",
    "\n",
    "# for each document in the test combined test corpus, call dict_langid with typical_words_tun and then with typical_words_tun\n",
    "# dict_langid(typical_words_tun, doc)\n",
    "# dict_langid(typical_words_ara, doc)\n",
    "# ...\n",
    "sumARA=0\n",
    "sumTUN=0\n",
    "for doc in tun_corpus_clean_test:\n",
    "    if (dict_langid(typical_words_ara,doc) > dict_langid(typical_words_tun,doc)):\n",
    "        sumARA = sumARA+1\n",
    "    else:\n",
    "        sumTUN = sumTUN+1\n",
    "if(sumARA >= sumTUN):\n",
    "    Lang='ARA'\n",
    "else:\n",
    "    Lang='TUN'\n",
    "print(Lang)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARA\n"
     ]
    }
   ],
   "source": [
    "sumARA=0\n",
    "sumTUN=0\n",
    "for doc in ara_corpus_clean_test:\n",
    "    if (dict_langid(typical_words_ara,doc) > dict_langid(typical_words_tun,doc)):\n",
    "        sumARA = sumARA+1\n",
    "    else:\n",
    "        sumTUN = sumTUN+1\n",
    "if(sumARA >= sumTUN):\n",
    "    Lang='ARA'\n",
    "else:\n",
    "    Lang='TUN'\n",
    "print(Lang)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "# Reference: see scikit-learn documentation\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a language classifier using supervised learning\n",
    "- **Step 0**: Divide each corpus into a training corpus (70%) and test corpus (30%).\n",
    "- **Step 1**: Create a data frame called ``train_df`` that has two columns: 'document' and 'language'. The 'document' column contains the two corpora concatenated together. The values in the '' column should be 'TUN' and 'ARA'.  Repeat the same thing for the ``test_df``.\n",
    "- **Step 2**: Convert the training documents into numeric feature vectors using the BOW-tfidf method with **character ngrams**.\n",
    "- **Step 3**: Create a language classifier using Naive Bayes method (tfidf version).\n",
    "- **Step 4**: Evaluate performance of this classifier based on the test corpus -- calculate classification accuracy, precision, recall, F1, and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 6\n",
    "Implement each step by carefully following the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 just use the same variables that you created in Step 1 of the previous part.\n",
    "# Nothing to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>كوثر ياسر سمنت العام هذا توا راهو موش معقول مم...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>انا نحبها و خاصة في شوفلي حل</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ولاه ماسط ولا يعرف اظحك لا شيء</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>التمثيل فاشل و في الكلام فاشل و الحوارات فاشل ...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>من احسن ما انجبت بلادي من ممثلين وليس مثليين هه</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document language\n",
       "0  كوثر ياسر سمنت العام هذا توا راهو موش معقول مم...      TUN\n",
       "1                       انا نحبها و خاصة في شوفلي حل      TUN\n",
       "2                     ولاه ماسط ولا يعرف اظحك لا شيء      TUN\n",
       "3  التمثيل فاشل و في الكلام فاشل و الحوارات فاشل ...      TUN\n",
       "4    من احسن ما انجبت بلادي من ممثلين وليس مثليين هه      TUN"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1   create 2 data frames called train_df and test_df (as explained above)\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "# create data frame\n",
    "train_df = pd.DataFrame({'document':[], 'language':[]})\n",
    "\n",
    "# fill the language column\n",
    "train_df.language = pd.Series(['TUN']*len(tun_corpus_clean_train) + ['ARA']*len(tun_corpus_clean_train))\n",
    "\n",
    "\n",
    "# fill the document column -- CONCATENATE the TUN CORPUS and ARA CORPUS\n",
    "train_df.document = pd.Series(tun_corpus_clean_train + ara_corpus_clean_train)\n",
    "\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الجبناء والسفهاء لا يصنعون التاريخ وكذلك اعلام...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>امراة تفرض احترامها</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ينعل ابوك وابو اصلك وابو امك ياكلبة ياخامجة يا...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>مش تسخف تتزعبن</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ماصطة لاصطة حتي شي توا يجيكم شكون اندمكم ع الضحك</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document language\n",
       "0  الجبناء والسفهاء لا يصنعون التاريخ وكذلك اعلام...      TUN\n",
       "1                                امراة تفرض احترامها      TUN\n",
       "2  ينعل ابوك وابو اصلك وابو امك ياكلبة ياخامجة يا...      TUN\n",
       "3                                     مش تسخف تتزعبن      TUN\n",
       "4   ماصطة لاصطة حتي شي توا يجيكم شكون اندمكم ع الضحك      TUN"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame({'document':[], 'language':[]})\n",
    "test_df.language = pd.Series(['TUN']*len(tun_corpus_clean_test) + ['ARA']*len(tun_corpus_clean_test))\n",
    "test_df.document = pd.Series(tun_corpus_clean_test + ara_corpus_clean_test)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the number of rows in this data frame = sum of the number of documents in each corpus.  \n",
    "try:\n",
    "    \n",
    "    assert train_df.shape[0] == len(tun_corpus_clean_train) + len(ara_corpus_clean_train) \n",
    "    assert train_df.shape[1] == 2\n",
    "except AssertionError:\n",
    "    print('An error occurred')\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(train_df.language.nunique() == 2 and train_df.language.unique() == ['TUN', 'ARA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2  Convert the training documents into numeric feature vectors using the BOW-tfidf method with character ngrams.\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "n = 3   # hyperparameter for of character ngrams ; you can change it if you want but n=3 is a reaonable value ...\n",
    "\n",
    "# Create an instance of TfidfVectorizer class with analyzer = 'char' so that it generates bag of characters and not bag of words\n",
    "bow_model_char = TfidfVectorizer(analyzer='char', ngram_range=(1,n), max_df =0.9, min_df=0.1)\n",
    "\n",
    "# Call fit method with the combined training corpus\n",
    "bow_model_char.fit(train_df.document)\n",
    "\n",
    "# Create DTM matrix of the combined training corpus and test corpus\n",
    "dtm_Train=bow_model_char.transform(train_df.document)\n",
    "dtm_Test=bow_model_char.transform(test_df.document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TUN', 'TUN', 'TUN', ..., 'ARA', 'ARA', 'ARA'], dtype='<U3')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3   -- see official documentation of MultinomialNB in scikit-learn\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "nb_model.fit(dtm_Train,train_df.language)\n",
    "\n",
    "nb_model.predict(dtm_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9446290143964563"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4   Use the same source code as in Step 4 of the previous part.\n",
    "print(\"The accuracy score : \")\n",
    "accuracy_score(test_df.language,nb_model.predict(dtm_Test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9447075490791338"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The precision score :\")\n",
    "precision_score(test_df.language,nb_model.predict(dtm_Test), average=\"macro\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9446290143964562"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The recall: \")\n",
    "recall_score(test_df.language,nb_model.predict(dtm_Test), average=\"macro\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9446265696803794"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The f1: \")\n",
    "f1_score(test_df.language,nb_model.predict(dtm_Test), average='macro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The confusion matrix : \")\n",
    "confusion_matrix(test_df.language,nb_model.predict(dtm_Test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
