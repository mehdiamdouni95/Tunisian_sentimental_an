{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline:**\n",
    "1. Read raw data\n",
    "2. Clean data\n",
    "3. Try existing language identification libraries\n",
    "4. Construct our own dictionary-based language classifier\n",
    "5. Construct our own language classifier using supervised learning\n",
    "\n",
    "**What you need to do:** \n",
    "- Read and execute the source code below and answer the questions in **EXERCISE 1 - EXERCISE 6**. \n",
    "- **Submit** the modifiled file ``TD2.ipynb`` on google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set the font size of plots\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_files = ['C:/Users/hichem/Desktop/td tm/langid_data_TUN-AR.txt', 'C:/Users/hichem/Desktop/td tm/langid_data_ARA.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    print('Reading file ' + filename + \"...\")\n",
    "    with open(filename, \"r\", encoding='utf8') as textfile:\n",
    "        L = []\n",
    "        for line in textfile:\n",
    "            L.append(line.strip())\n",
    "        print('File contains ', len(L), \"lines.\\n\")\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file C:/Users/hichem/Desktop/td tm/langid_data_TUN-AR.txt...\n",
      "File contains  13932 lines.\n",
      "\n",
      "Reading file C:/Users/hichem/Desktop/td tm/langid_data_ARA.txt...\n",
      "File contains  21787 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tun_corpus = read_text_file(corpus_files[0])\n",
    "ara_corpus = read_text_file(corpus_files[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "For language identification, we usually only need to remove non-word characters and replace them with spaces. But since our corpus contains social media text, a few other operations are necessary.\n",
    "\n",
    "- Remove non-word symbols (punctuation, math symbols, emoticons, URLs, hashtags, etc.).\n",
    "- Replace punctuation and white space with a single space.\n",
    "- Normalize word elongations and word repetitions.\n",
    "- Remove documents that contain a large fraction of latin characters (some documents contain english or french words).\n",
    "- Remove very short documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This just a simple js jshd jhsd js dh asjh test'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# regexp for word elongation: matches 3 or more repetitions of a word character.\n",
    "two_plus_letters_RE = re.compile(r\"(\\w)\\1{1,}\", re.DOTALL)\n",
    "three_plus_letters_RE = re.compile(r\"(\\w)\\1{2,}\", re.DOTALL)\n",
    "# regexp for repeated words\n",
    "two_plus_words_RE = re.compile(r\"(\\w+\\s+)\\1{1,}\", re.DOTALL)\n",
    "\n",
    "\n",
    "def cleanup_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "\n",
    "    # Remove user mentions of the form @username\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "    \n",
    "    # Replace special html-encoded characters with their ASCII equivalent, for example: &#39 ==> '\n",
    "    #if re.search(\"&#\",text):\n",
    "        #text = html.unescape(text)\n",
    "\n",
    "    # Remove special useless characters such as _x000D_\n",
    "    text = re.sub(r'_[xX]000[dD]_', '', text)\n",
    "\n",
    "    # Replace all non-word characters (such as emoticons, punctuation, end of line characters, etc.) with a space\n",
    "    text = re.sub('[\\W_]', ' ', text)\n",
    "\n",
    "    # Remove redundant white spaces\n",
    "    text = text.strip()\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "\n",
    "    # normalize word elongations (characters repeated more than twice)\n",
    "    text = two_plus_letters_RE.sub(r\"\\1\\1\", text)\n",
    "\n",
    "    # remove repeated words\n",
    "    text = two_plus_words_RE.sub(r\"\\1\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# unit test of this function\n",
    "cleanup_text(\"This is just a simple. js .... jshd)jhsd__js--dh \\n\\n asjh\\n test !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 1\n",
    "\n",
    "Do the following operations for **each** corpus. Store the new corpora in new variables called ``tun_corpus_clean`` and ``ara_corpus_clean``\n",
    "1. Clean up each document in the corpus using the ``cleanup_text`` function given above.\n",
    "2. Remove all documents that contain a large fraction of latin characters (for example more than 70%).  **Hint**: use a regular expression with pattern [a-zA-Z]\n",
    "3. Remove very short documents (containing less than 10 characters, for e.g.).\n",
    "4. Display the number of documents in the clean corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12390 21350\n"
     ]
    }
   ],
   "source": [
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "tun_corpus_clean = [cleanup_text(doc) for doc in tun_corpus]\n",
    "ara_corpus_clean = [cleanup_text(doc) for doc in ara_corpus]\n",
    "\n",
    "\n",
    "#Remove all documents that contain a large fraction of latin characters (for example more than 70%).\n",
    "tun_corpus_clean = [doc for doc in tun_corpus_clean if len(re.findall('[a-zA-Z]',doc))/len(doc)<0.7]\n",
    "ara_corpus_clean = [doc for doc in ara_corpus_clean if len(re.findall('[a-zA-Z]',doc))/len(doc)<0.7]\n",
    "\n",
    "#Remove very short documents\n",
    "tun_corpus_clean = [doc for doc in tun_corpus_clean if len(doc)>=10]\n",
    "ara_corpus_clean = [doc for doc in ara_corpus_clean if len(doc)>=10]\n",
    "\n",
    "#Display the number of documents in the clean corpus\n",
    "print(len(tun_corpus_clean), len(ara_corpus_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try existing language identification libraries\n",
    "- In TD1, we used NLTK's textcat library.\n",
    "- In this TD, we will use another 2 libraries (``langdetect`` and ``langid``) which are actually more accurate than ``textcat``. However, as you will see they "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load special libraries for language identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = \"This is just a simple test !!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[en:0.8571406756117597, et:0.1428571667925181]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# langdetect library\n",
    "\n",
    "import langdetect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "try:\n",
    "    res = langdetect.detect_langs(test_doc)   # LANGDETECT\n",
    "    #res = langdetect.detect(test_doc) \n",
    "except LangDetectException:\n",
    "    res = langdetect.language.Language(\"UNK\",0)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('en', 0.9999998819046767)\n",
      "[('en', 0.9999998819046767), ('br', 1.1536448337940777e-07), ('la', 1.4557736338637932e-09)]\n"
     ]
    }
   ],
   "source": [
    "# langid library\n",
    "\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "li = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "print(li.classify(test_doc))\n",
    "print(li.rank(test_doc)[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eng'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK textcat library - JUST IN CASE\n",
    "from nltk.classify.textcat import TextCat\n",
    "\n",
    "\n",
    "# create class instance\n",
    "tc = TextCat()\n",
    "tc.guess_language(test_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ``langdetect`` on a small sample of Tunisian Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a small random sample of 5000 documents from the corpus\n",
    "n = 5000\n",
    "random_indices = np.random.choice(np.arange(len(tun_corpus_clean)), n, replace=False)\n",
    "small_corpus = [tun_corpus_clean[i] for i in random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_langdetect = []\n",
    "\n",
    "for doc in small_corpus:\n",
    "    try:\n",
    "        res_langdetect.append(langdetect.detect_langs(doc))\n",
    "    except LangDetectException:\n",
    "        res_langdetect.append([langdetect.language.Language(\"UNK\",0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_corpus),len(res_langdetect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>language</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>يا ليت عندنا 4 منك في تونس يا جعفور متربي ثقة ...</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>الشيب و العيب</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>المحامي الفاشل بن خليل اللي ما ربح حتي قضية ري...</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اكيد باش تقري الكومنتارات بعد البرنامج نقلها ر...</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الله لا تباركلك يا سامي الفهري يا رب يورينا في...</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document language  probability\n",
       "0  يا ليت عندنا 4 منك في تونس يا جعفور متربي ثقة ...       ar     0.999998\n",
       "1                                      الشيب و العيب       ar     0.999999\n",
       "2  المحامي الفاشل بن خليل اللي ما ربح حتي قضية ري...       ar     0.999998\n",
       "3  اكيد باش تقري الكومنتارات بعد البرنامج نقلها ر...       ar     0.999997\n",
       "4  الله لا تباركلك يا سامي الفهري يا رب يورينا في...       ar     0.999996"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's put the results in a data frame for ease of manipulation\n",
    "\n",
    "def foo1(x):\n",
    "    u = str(x).split(':')\n",
    "    return [u[0],float(u[1])]\n",
    "L = [[small_corpus[i]]+foo1(x[0]) for i,x in enumerate(res_langdetect)]\n",
    "df = pd.DataFrame(L)\n",
    "df.columns = ['document', 'language','probability']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 3\n",
    "1. Plot the distribution of languages (call value_counts() on the ``language`` column of ``df``)\n",
    "2. Plot the histogram of probabilities (call plot(kind='hist') on the ``probability`` column of ``df``)\n",
    "3. What are the top 2 languages identified in this corpus?\n",
    "4. For what fraction of documents is this language identifier more than 80% confident in its decision? (i.e. probability > 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENTER YOUR ANSWERS BELOW\n",
    "\n",
    "3. Arabic and farsi <br>\n",
    "4.  fraction of documents is = 0.9824\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14aa74ead68>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD9CAYAAAC1DKAUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAD5lJREFUeJzt3X+s3XV9x/HnixZ/TinIhZCWrDj7hzjnj12BzCVTMFBwW/lDFsyilWCaLGyyZctEs4QfyoZ/bGxmE9MIWTXbgDgdjSNig5DNLAqtIgiMtOIP7krgmlamc7AU3/vjfCqHett77u3pPdx+no/k5Hy/7+/nfM/7e3JzX+f745yTqkKS1J9jJt2AJGkyDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSp1aOMijJd4EfAc8C+6pqOskJwC3AWuC7wO9U1d4kAf4GuAD4CfC+qvp6W89G4M/aaj9aVVsO9bwnnnhirV27doGbJEl927Fjxw+qamq+cSMFQPP2qvrB0PwVwJ1VdV2SK9r8B4HzgXXtdiZwA3BmC4wrgWmggB1JtlbV3oM94dq1a9m+ffsCWpQkJfneKOMO5xDQBmD/O/gtwIVD9U/XwFeBVUlOAc4DtlXVnvZPfxuw/jCeX5J0GEYNgAK+lGRHkk2tdnJVPQ7Q7k9q9dXAY0OPnWm1g9WfJ8mmJNuTbJ+dnR19SyRJCzLqIaC3VtXuJCcB25L85yHGZo5aHaL+/ELVZmAzwPT0tF9VKklHyEh7AFW1u90/CXweOAN4oh3aod0/2YbPAKcOPXwNsPsQdUnSBMwbAElenuQV+6eBc4FvAVuBjW3YRuC2Nr0VeG8GzgKeaoeI7gDOTXJ8kuPbeu4Y69ZIkkY2yiGgk4HPD67uZCXwj1X1xST3ArcmuRT4PnBRG387g0tAdzG4DPQSgKrak+QjwL1t3DVVtWdsWyJJWpC8kH8RbHp6urwMVJIWJsmOqpqeb5yfBJakTi3kg2BHhbVX/OukWxjJd69756RbkHSUcw9AkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1KmRAyDJiiTfSPKFNn9akq8l2ZnkliQvavUXt/ldbfnaoXV8qNUfSXLeuDdGkjS6hewBXA48PDT/MeD6qloH7AUubfVLgb1V9Rrg+jaOJKcDFwOvA9YDn0iy4vDalyQt1kgBkGQN8E7gU20+wNnAZ9uQLcCFbXpDm6ctP6eN3wDcXFXPVNV3gF3AGePYCEnSwo26B/DXwJ8CP23zrwJ+WFX72vwMsLpNrwYeA2jLn2rjf1af4zE/k2RTku1Jts/Ozi5gUyRJCzFvACT5TeDJqtoxXJ5jaM2z7FCPea5Qtbmqpqtqempqar72JEmLtHKEMW8FfjvJBcBLgFcy2CNYlWRle5e/Btjdxs8ApwIzSVYCxwF7hur7DT9GkrTE5t0DqKoPVdWaqlrL4CTul6vqd4G7gHe1YRuB29r01jZPW/7lqqpWv7hdJXQasA64Z2xbIklakFH2AA7mg8DNST4KfAO4sdVvBD6TZBeDd/4XA1TVg0luBR4C9gGXVdWzh/H8kqTDsKAAqKq7gbvb9KPMcRVPVT0NXHSQx18LXLvQJiVJ4+cngSWpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1at4ASPKSJPck+WaSB5Nc3eqnJflakp1JbknyolZ/cZvf1ZavHVrXh1r9kSTnHamNkiTNb5Q9gGeAs6vqDcAbgfVJzgI+BlxfVeuAvcClbfylwN6qeg1wfRtHktOBi4HXAeuBTyRZMc6NkSSNbt4AqIEft9lj262As4HPtvoW4MI2vaHN05afkyStfnNVPVNV3wF2AWeMZSskSQs20jmAJCuS3Ac8CWwDvg38sKr2tSEzwOo2vRp4DKAtfwp41XB9jscMP9emJNuTbJ+dnV34FkmSRjJSAFTVs1X1RmANg3ftr51rWLvPQZYdrH7gc22uqumqmp6amhqlPUnSIizoKqCq+iFwN3AWsCrJyrZoDbC7Tc8ApwK05ccBe4brczxGkrTERrkKaCrJqjb9UuAdwMPAXcC72rCNwG1temubpy3/clVVq1/crhI6DVgH3DOuDZEkLczK+YdwCrClXbFzDHBrVX0hyUPAzUk+CnwDuLGNvxH4TJJdDN75XwxQVQ8muRV4CNgHXFZVz453cyRJo5o3AKrqfuBNc9QfZY6reKrqaeCig6zrWuDahbcpSRo3PwksSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdmjcAkpya5K4kDyd5MMnlrX5Ckm1Jdrb741s9ST6eZFeS+5O8eWhdG9v4nUk2HrnNkiTNZ5Q9gH3AH1fVa4GzgMuSnA5cAdxZVeuAO9s8wPnAunbbBNwAg8AArgTOBM4ArtwfGpKkpTdvAFTV41X19Tb9I+BhYDWwAdjShm0BLmzTG4BP18BXgVVJTgHOA7ZV1Z6q2gtsA9aPdWskSSNb0DmAJGuBNwFfA06uqsdhEBLASW3YauCxoYfNtNrB6pKkCRg5AJL8AvDPwB9W1X8faugctTpE/cDn2ZRke5Lts7Ozo7YnSVqgkQIgybEM/vn/Q1V9rpWfaId2aPdPtvoMcOrQw9cAuw9Rf56q2lxV01U1PTU1tZBtkSQtwChXAQW4EXi4qv5qaNFWYP+VPBuB24bq721XA50FPNUOEd0BnJvk+Hby99xWkyRNwMoRxrwVeA/wQJL7Wu3DwHXArUkuBb4PXNSW3Q5cAOwCfgJcAlBVe5J8BLi3jbumqvaMZSskSQs2bwBU1VeY+/g9wDlzjC/gsoOs6ybgpoU0KEk6MvwksCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROzRsASW5K8mSSbw3VTkiyLcnOdn98qyfJx5PsSnJ/kjcPPWZjG78zycYjszmSpFGNsgfw98D6A2pXAHdW1TrgzjYPcD6wrt02ATfAIDCAK4EzgTOAK/eHhiRpMuYNgKr6N2DPAeUNwJY2vQW4cKj+6Rr4KrAqySnAecC2qtpTVXuBbfx8qEiSltBizwGcXFWPA7T7k1p9NfDY0LiZVjtYXZI0IeM+CZw5anWI+s+vINmUZHuS7bOzs2NtTpL0nMUGwBPt0A7t/slWnwFOHRq3Bth9iPrPqarNVTVdVdNTU1OLbE+SNJ/FBsBWYP+VPBuB24bq721XA50FPNUOEd0BnJvk+Hby99xWkyRNyMr5BiT5J+BtwIlJZhhczXMdcGuSS4HvAxe14bcDFwC7gJ8AlwBU1Z4kHwHubeOuqaoDTyxLkpbQvAFQVe8+yKJz5hhbwGUHWc9NwE0L6k6SdMT4SWBJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSp1ZOugEtc1cdN+kORnPVU5PuQHrBcQ9Akjq15AGQZH2SR5LsSnLFUj+/JGlgSQMgyQrg74DzgdOBdyc5fSl7kCQNLPU5gDOAXVX1KECSm4ENwENL3If0gvT6La+fdAsjeWDjA5NuQWOw1IeAVgOPDc3PtJokaYkt9R5A5qjV8wYkm4BNbfbHSR454l0dvhOBH4xzhfnYONe27Iz99eTquf70ujH+v8/3dft6jv9v88j4xVEGLXUAzACnDs2vAXYPD6iqzcDmpWzqcCXZXlXTk+7jaOHrOV6+nuNztL2WS30I6F5gXZLTkrwIuBjYusQ9SJJY4j2AqtqX5PeBO4AVwE1V9eBS9iBJGljyTwJX1e3A7Uv9vEfYsjpktQz4eo6Xr+f4HFWvZapq/lGSpKOOXwUhSZ0yACSpUwbAIiQ5JsmvTboPSTocBsAiVNVPgb+cdB/SgZKsSPJHk+7jaJLk2CQfSPLZdvuDJMdOuq9x8CTwIiW5Grgf+Fz5Ih6WJOuAv2DwBYEv2V+vqldPrKllLMndVfW2SfdxtEjyKeBYYEsrvQd4tqreP7muxsMAWKQkPwJeDuwDnmbwNRdVVa+caGPLUJKvAFcC1wO/BVzC4G/zyok2tkwluRY4DrgF+J/99ar6+sSaWsaSfLOq3jBfbTnyF8EWqapekeQEYB1D71q1KC+tqjuTpKq+B1yV5N8ZhIIWbv/5qavbfRh859bZk2ln2Xs2yS9V1bcBkrwaeHbCPY2FAbBISd4PXM7g+4zuA84C/gM4Z5J9LVNPJzkG2Nk+Kf5fwEkT7mk5u3uOmrv6i/cnwF1JHm3zaxnspS57ngRevMuBtwDfq6q3A29ieXxL4AtGks+0yduAlwEfAH6VwTHWjZPq6yjw46HbPmA9g39aWpxXAb/M4O/zTuBh4Kj4kWnPASxSknur6i1J7gPOrKpnktxXVW+cdG/LRZKHGPw63FbgbRzwdeFVtWcCbR11krwY2FpV5026l+Uoyf1V9StJfh34cwZXAH64qs6ccGuHzUNAizeTZBXwL8C2JHs54KutNa9PAl8EXg3s4Llj1fvvvQpoPF6Gr+Xh2H+8/53AJ6vqtiRXTbCfsXEPYAyS/AaDqy6+WFX/N+l+lpskN1TV7026j6NFkgd47pj/CmAKuKaq/nZyXS1fSb7A4LzUOxgcovxf4J6j4SogA0A6yiQZ/jWofcATVbVvUv0sd0lexuA8ygNVtTPJKcDrq+pLE27tsBkAktQprwKSpE4ZAJLUKQNAkjplAEhSpwwASerU/wO0rv4+Q7R0cAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ENTER YOUR SOURCE CODE BELOW\n",
    "\n",
    "#1\n",
    "pd.Series(df.language).value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14aa76d2550>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2\n",
    "pd.Series(df.probability).plot(kind='hist', title='document length')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9784\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "fraction= len([doc for i,doc in enumerate(df.document) if df.probability[i]>0.8])/n\n",
    "print(fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EXERCISE 4 (homework - skip this during class)\n",
    "\n",
    "1. Test both libraries (``langdetect`` and ``langid``) on the **entire** ``ara_corpus_clean`` corpus.\n",
    "2. Which library seems to be more accurate for the Arabic language? Justify based on visual inspection of the results.\n",
    "3. Design a new language identification method that combines both libraries -- basically just use the result corresponding to the higher probability.\n",
    "4. Is this new method more accurate than the individual methods? Again, justify your answer based on visual inspection of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "1. See code below.\n",
    "2. the langdect seems to be more accurate because mean of probability of langdetect(0.997335) is greater that that of langid(0.992880)\n",
    "3. see code below.\n",
    "4. yes. when visualising the probability discription of this combined method we find (mean=0.999339, min=0.571425, 25%=1, 50%=1, 75%=1, max=1) which is more accurate than the two individual methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. \n",
    "# langdetect\n",
    "res_langdetect_entire = []\n",
    "\n",
    "for doc in ara_corpus_clean:\n",
    "    try:\n",
    "        res_langdetect_entire.append(langdetect.detect_langs(doc))\n",
    "    except LangDetectException:\n",
    "        res_langdetect_entire.append([langdetect.language.Language(\"UNK\",0)])\n",
    "\n",
    "# langid\n",
    "res_langid_entire = []\n",
    "for doc in ara_corpus_clean:\n",
    "    res_langid_entire.append(li.classify(doc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21350.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.997442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.027396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        probability\n",
       "count  21350.000000\n",
       "mean       0.997442\n",
       "std        0.027396\n",
       "min        0.000000\n",
       "25%        0.999997\n",
       "50%        0.999998\n",
       "75%        0.999999\n",
       "max        1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.langdetect\n",
    "L1=[foo1(x[0]) for i,x in enumerate(res_langdetect_entire)]\n",
    "df1 = pd.DataFrame(L1)\n",
    "df1.columns = ['language','probability']\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6b58bd721b54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#langid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_langid_entire\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'language'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'probability'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#langid\n",
    "df2 = pd.DataFrame(res_langid_entire)\n",
    "df2.columns = ['language','probability']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ar', 1.0],\n",
       " ['ar', 0.9999999999999949],\n",
       " ['ar', 0.9999999999726172],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999977735088371],\n",
       " ['ar', 0.99999999999087],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999969667056725],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999957195156082],\n",
       " ['ar', 0.9999980558306163],\n",
       " ['ar', 0.9999999999243518],\n",
       " ['ar', 0.9999977571653603],\n",
       " ['ar', 0.9999974703350184],\n",
       " ['ar', 0.9999979057089576],\n",
       " ['ar', 0.9999999940391424],\n",
       " ['ar', 0.9999999845007872],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999975760871341],\n",
       " ['ar', 0.9999965443915516],\n",
       " ['ar', 0.9999999999771316],\n",
       " ['ar', 0.9999999999999989],\n",
       " ['ar', 0.9999990134442406],\n",
       " ['ar', 0.9999973391380288],\n",
       " ['ar', 0.9999997584097751],\n",
       " ['ar', 0.9999943217965727],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999968927],\n",
       " ['ar', 0.999998311493066],\n",
       " ['ar', 0.9999968116049902],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999990428],\n",
       " ['ar', 0.9999981769513133],\n",
       " ['ar', 0.9999999968174536],\n",
       " ['ar', 0.9999999999906253],\n",
       " ['ar', 0.9999999999999958],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999971364541337],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999998507119],\n",
       " ['fa', 0.9911079081744665],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999964347865522],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999982],\n",
       " ['ar', 0.9999956823281786],\n",
       " ['ar', 0.9871188984535091],\n",
       " ['ur', 0.9999957645243717],\n",
       " ['ar', 0.999996840062833],\n",
       " ['ar', 0.9999999999999836],\n",
       " ['ar', 0.9999984542511257],\n",
       " ['ar', 0.999997298054409],\n",
       " ['ar', 0.9999999753554097],\n",
       " ['ar', 0.9999999999985081],\n",
       " ['ar', 0.970341032195038],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999970026428667],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999990802200127],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999995132523947],\n",
       " ['ar', 0.9999955077273907],\n",
       " ['ar', 0.9999999999999989],\n",
       " ['ar', 0.9999991701734527],\n",
       " ['ar', 0.9999999906721947],\n",
       " ['ar', 0.9999999999999865],\n",
       " ['fa', 0.9999999988940329],\n",
       " ['ar', 0.9999999981202761],\n",
       " ['ar', 0.9999968830285926],\n",
       " ['ar', 0.9999999999999718],\n",
       " ['ar', 0.9999994464155546],\n",
       " ['ar', 0.9999935099003574],\n",
       " ['ar', 0.9999999999417846],\n",
       " ['fa', 0.9999969495897834],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999980351440543],\n",
       " ['ar', 0.9999999999958817],\n",
       " ['ar', 0.9999999993324864],\n",
       " ['ar', 0.9999965455646784],\n",
       " ['ar', 0.999996455209124],\n",
       " ['ar', 0.9999999422707826],\n",
       " ['ur', 0.705682804899199],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999179261053],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999854927193],\n",
       " ['ar', 0.999996082218826],\n",
       " ['ar', 0.9999988051760971],\n",
       " ['ar', 0.9999968962734588],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999981620856364],\n",
       " ['ar', 0.9999999987454051],\n",
       " ['ar', 0.9999999860071356],\n",
       " ['ar', 0.9999971837435224],\n",
       " ['ar', 0.9999999984946615],\n",
       " ['ar', 0.9999966396634313],\n",
       " ['ar', 0.9999999994264639],\n",
       " ['fa', 0.9999999611142449],\n",
       " ['ar', 0.9999971334189064],\n",
       " ['fa', 0.9999971721354597],\n",
       " ['ar', 0.9999964411416251],\n",
       " ['ar', 0.9999975612190203],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999977497041423],\n",
       " ['ar', 0.9999968798675647],\n",
       " ['ar', 0.9999959840811419],\n",
       " ['ar', 0.9999991349586463],\n",
       " ['ar', 0.9999999999999465],\n",
       " ['ar', 0.9999999999988869],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999972957],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999987],\n",
       " ['ar', 0.9999998456059319],\n",
       " ['ar', 0.9999975664981945],\n",
       " ['ar', 0.9999906961689555],\n",
       " ['ur', 0.8571407855530151],\n",
       " ['ar', 0.9999981176172141],\n",
       " ['ar', 0.9999999999901226],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ur', 0.9117195484017651],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9831463790107313],\n",
       " ['ar', 0.9999994777473951],\n",
       " ['ar', 0.9999999990684236],\n",
       " ['ar', 0.9999999993720383],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999537672],\n",
       " ['ar', 0.9999997987107924],\n",
       " ['ar', 0.9999971916271051],\n",
       " ['ar', 0.9999956451979789],\n",
       " ['ar', 0.9999999964635513],\n",
       " ['ar', 0.9999999999029123],\n",
       " ['ar', 0.9999973110404974],\n",
       " ['ar', 0.9999963056797402],\n",
       " ['ar', 0.9999974592396768],\n",
       " ['ar', 0.9999999999951747],\n",
       " ['ar', 0.9999963071678013],\n",
       " ['ar', 0.9999999914993337],\n",
       " ['ar', 0.9999972724650174],\n",
       " ['ar', 0.999999330451103],\n",
       " ['ar', 0.9999999998645428],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999971103680272],\n",
       " ['ar', 0.9999999999974922],\n",
       " ['ar', 0.9999999848800585],\n",
       " ['ar', 0.999999939516356],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999998492795684],\n",
       " ['ar', 0.9999979559649211],\n",
       " ['ar', 0.9999973360141973],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999999999142],\n",
       " ['ar', 0.9999972848206162],\n",
       " ['ar', 0.999999999999835],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999894526906],\n",
       " ['ar', 0.9999979049965203],\n",
       " ['ar', 0.9999979921958707],\n",
       " ['ar', 0.9999999404148272],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999999999986],\n",
       " ['ar', 0.9881150974122591],\n",
       " ['ar', 0.9999951329464529],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999998927306368],\n",
       " ['ar', 0.9999958966424798],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999943051516],\n",
       " ['ar', 0.9999999999758455],\n",
       " ['ar', 0.9999999988403834],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999979419191056],\n",
       " ['ar', 0.9999999999999967],\n",
       " ['ar', 0.8571399068615495],\n",
       " ['ar', 0.9999984019033921],\n",
       " ['ar', 0.9999999999958185],\n",
       " ['ar', 0.9999999999999989],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999999968679],\n",
       " ['ar', 0.9999999999999987],\n",
       " ['ar', 0.9999999997623326],\n",
       " ['ar', 0.9999999975844858],\n",
       " ['ar', 0.9999983623099654],\n",
       " ['ar', 0.9999960494692868],\n",
       " ['ar', 0.9999999998052664],\n",
       " ['ar', 0.9999999999999778],\n",
       " ['ar', 0.9999999999993436],\n",
       " ['ar', 0.9999999992994548],\n",
       " ['ar', 0.9999999977268219],\n",
       " ['ar', 0.9999989743104709],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999994131152054],\n",
       " ['ar', 0.9999999965763977],\n",
       " ['ar', 0.9999999999998501],\n",
       " ['ar', 0.999997740689031],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999971754671664],\n",
       " ['ar', 0.9999966556135319],\n",
       " ['ar', 0.9999971172940949],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 0.9999986138121809],\n",
       " ['ar', 0.9999986981170538],\n",
       " ['ar', 0.999994195284955],\n",
       " ['ar', 0.999998246481492],\n",
       " ['ar', 0.9999980435708294],\n",
       " ['ar', 0.999999999999412],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999956857180958],\n",
       " ['ar', 0.9999968331035552],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999999993997],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999985428487624],\n",
       " ['ar', 0.9999973880348254],\n",
       " ['ar', 0.999995506724817],\n",
       " ['ar', 0.999999744695809],\n",
       " ['ar', 0.9999999995984592],\n",
       " ['ar', 0.9999978401376893],\n",
       " ['ar', 0.9999988746758426],\n",
       " ['ar', 0.9999976512149278],\n",
       " ['ar', 0.9999983084613961],\n",
       " ['ar', 0.9999956442863976],\n",
       " ['ar', 0.999997599920224],\n",
       " ['ar', 0.9999999999999754],\n",
       " ['ar', 0.9999973809617364],\n",
       " ['ar', 0.9999960676257482],\n",
       " ['ur', 0.992158077351994],\n",
       " ['ar', 0.9999969191488218],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999995662563],\n",
       " ['ar', 0.9999999998250892],\n",
       " ['ar', 0.9999859929037989],\n",
       " ['ar', 0.9999955371924488],\n",
       " ['ar', 0.9999992927129345],\n",
       " ['ar', 0.9999996905784754],\n",
       " ['ar', 0.9999972496737768],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999976356553523],\n",
       " ['ar', 0.9999999999999833],\n",
       " ['ar', 0.9999960599067999],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999995781662351],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999493254993],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999967725460496],\n",
       " ['ar', 0.9999988911657285],\n",
       " ['ar', 1.0],\n",
       " ['ur', 0.9999976273513367],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999980709],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999962291458317],\n",
       " ['ar', 0.9999974765917844],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999975619072906],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999689773419],\n",
       " ['ar', 0.9999999999999993],\n",
       " ['ar', 0.9999999999998368],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 0.9999998141772684],\n",
       " ['ar', 0.9999999960507653],\n",
       " ['ar', 0.9999999999090383],\n",
       " ['ar', 0.9999980392733101],\n",
       " ['ar', 0.9999987197486531],\n",
       " ['ar', 0.9999994888657059],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999865739],\n",
       " ['ar', 0.9999914846268909],\n",
       " ['ar', 0.9999997287716466],\n",
       " ['ar', 0.999999999991551],\n",
       " ['ar', 0.9999994270166774],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999977389221222],\n",
       " ['ar', 0.9999999993248718],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999979489286843],\n",
       " ['ar', 0.9985336052908926],\n",
       " ['ar', 0.9999999926186227],\n",
       " ['ar', 0.9999986378644508],\n",
       " ['ar', 0.9999962299345407],\n",
       " ['ar', 0.9999999942096658],\n",
       " ['ar', 0.999994972288111],\n",
       " ['fa', 0.9999971755998853],\n",
       " ['ar', 0.9999974908179172],\n",
       " ['fa', 0.9999977669526804],\n",
       " ['ar', 0.9999999999998448],\n",
       " ['ar', 0.9999999963827646],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999981894],\n",
       " ['ar', 0.999999965267801],\n",
       " ['ar', 0.9999999999999538],\n",
       " ['ar', 0.9999999999999996],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999997269],\n",
       " ['ar', 0.9999999977498455],\n",
       " ['ar', 0.9999999999999987],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999958],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999952935358971],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999849],\n",
       " ['ar', 0.9999999999988225],\n",
       " ['ar', 0.9717280196573586],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999982905060613],\n",
       " ['ar', 0.9999959608312292],\n",
       " ['ar', 0.9999999999999394],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999968993607803],\n",
       " ['ar', 0.9999986273467963],\n",
       " ['ar', 0.999999999997162],\n",
       " ['ar', 0.9999997195215006],\n",
       " ['ur', 0.5714264559286041],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999997656239811],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999655156],\n",
       " ['ar', 0.9999960539117866],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999958419177488],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999929399],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999407009931295],\n",
       " ['ar', 0.9999999999298179],\n",
       " ['ar', 0.999999210688562],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999140271302],\n",
       " ['ar', 0.9999999999055189],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999984788142],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999942],\n",
       " ['ar', 0.9831771643672026],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999978268211425],\n",
       " ['ar', 0.9999972383877382],\n",
       " ['ar', 0.9999947245747377],\n",
       " ['ar', 0.9999975268909268],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999993],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999972819229315],\n",
       " ['ar', 0.9999999999999996],\n",
       " ['ar', 0.9999999999416718],\n",
       " ['ar', 0.9999994850270769],\n",
       " ['ar', 0.9999956894534601],\n",
       " ['ur', 0.9999996155172288],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999998152971793],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999982120544463],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999997389],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999510272],\n",
       " ['ar', 0.9999999999817182],\n",
       " ['fa', 0.9999976150113201],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999159982413],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999996136093678],\n",
       " ['ar', 0.9999982632814031],\n",
       " ['ar', 0.999994946659384],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9999765697838939],\n",
       " ['ar', 0.999999999980528],\n",
       " ['ar', 0.9999999999999469],\n",
       " ['ar', 0.9999999937506814],\n",
       " ['ar', 0.9999969436452897],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999983901167318],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999651],\n",
       " ['ar', 0.9999965708983783],\n",
       " ['ar', 0.999995663258671],\n",
       " ['ar', 0.9999996006305963],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999989430404],\n",
       " ['ar', 0.9999980462419189],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999996253694676],\n",
       " ['ar', 0.9999957724052402],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999999843413],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999998105007],\n",
       " ['ar', 0.9999966980651863],\n",
       " ['ar', 0.9999999369762396],\n",
       " ['ar', 0.9999999999992746],\n",
       " ['ar', 0.9999981937841056],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999978755848264],\n",
       " ['ar', 0.9999989066760911],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999980238829194],\n",
       " ['ar', 0.9999985550379029],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 0.9999968212665351],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999959934087421],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999674],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999973803],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999979417312734],\n",
       " ['ar', 0.9999994875652095],\n",
       " ['ar', 0.9999999998983835],\n",
       " ['ar', 0.9999999880559941],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999995345445],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999980502247492],\n",
       " ['ar', 0.9999959183848113],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 0.9999987523353663],\n",
       " ['ar', 0.9999984263831436],\n",
       " ['ar', 0.9999999999983307],\n",
       " ['ar', 0.9999999077934197],\n",
       " ['fa', 0.9999976582541055],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999936],\n",
       " ['ar', 0.9999996068089362],\n",
       " ['ar', 0.9999999779403888],\n",
       " ['ar', 0.9999981272775454],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999975189615761],\n",
       " ['ar', 0.9999999994514841],\n",
       " ['ar', 0.9999999999999993],\n",
       " ['ar', 0.999999399881228],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999512854],\n",
       " ['ar', 0.9999980537769044],\n",
       " ['ar', 0.9999967228574805],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 0.999999988869619],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999982207054454],\n",
       " ['ar', 0.9999996771709774],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999667435708],\n",
       " ['ar', 0.9999999352525193],\n",
       " ['ar', 0.9999975268148451],\n",
       " ['ar', 0.999999999998374],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9998974895524478],\n",
       " ['ar', 0.9999993773639553],\n",
       " ['ar', 0.9999999999951721],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999998068120787],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999908225],\n",
       " ['ar', 0.9999999998723468],\n",
       " ['ar', 0.9999958153532411],\n",
       " ['ar', 0.9999982171679574],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999997212950196],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.8295398429967694],\n",
       " ['ar', 0.9999999999995839],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999969824064858],\n",
       " ['ar', 0.9999999652159358],\n",
       " ['ar', 0.9999945551773064],\n",
       " ['ar', 0.9999998203003463],\n",
       " ['ar', 0.9999999999982747],\n",
       " ['ur', 0.9999953742559027],\n",
       " ['ar', 0.9999999999994351],\n",
       " ['ar', 0.9999999926296157],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999972197821682],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999483187],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 0.9999999999959053],\n",
       " ['ar', 0.9999999999999694],\n",
       " ['ar', 0.9999999745563763],\n",
       " ['ar', 0.9999999933946431],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.8506801886512612],\n",
       " ['ar', 0.9999975050393212],\n",
       " ['ar', 0.9999999999999971],\n",
       " ['ar', 0.9999957473582832],\n",
       " ['ar', 0.9999983998645825],\n",
       " ['fa', 0.9973869913927269],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999895368824],\n",
       " ['ar', 0.999999999637891],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999957628615012],\n",
       " ['ar', 0.999998002365964],\n",
       " ['ar', 0.9999999963261808],\n",
       " ['ar', 0.9999999997061517],\n",
       " ['ar', 0.9993637685521122],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999971412534374],\n",
       " ['ar', 0.9999993792975517],\n",
       " ['ar', 0.9999999985316506],\n",
       " ['ar', 0.9999954091405344],\n",
       " ['ar', 0.9999979443717136],\n",
       " ['ar', 0.9999999999999973],\n",
       " ['ar', 0.999999069071587],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999989679590986],\n",
       " ['ar', 0.999999999999968],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999993440876],\n",
       " ['ar', 0.9999999952587841],\n",
       " ['ar', 0.9999998320894066],\n",
       " ['ar', 0.9999978907241759],\n",
       " ['fa', 0.9998634407177176],\n",
       " ['ar', 0.9999999999999938],\n",
       " ['ar', 0.999997988760825],\n",
       " ['ar', 0.9999982064015626],\n",
       " ['ar', 0.9999999979487966],\n",
       " ['ar', 0.9672183204903707],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999988198],\n",
       " ['ar', 0.9999999999989957],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999969],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999990190576187],\n",
       " ['ar', 0.9999963827336866],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999990399768526],\n",
       " ['ar', 0.9999975584166018],\n",
       " ['ar', 0.9999999900282767],\n",
       " ['ar', 0.9999999996246172],\n",
       " ['ar', 0.9999964784329445],\n",
       " ['ar', 0.9999974259960274],\n",
       " ['ar', 0.9999972371392345],\n",
       " ['ar', 0.9999960954948817],\n",
       " ['ar', 0.9999969287834746],\n",
       " ['ar', 0.9999968304909562],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999933981083],\n",
       " ['ar', 0.9999999132682607],\n",
       " ['ar', 0.9999999992775135],\n",
       " ['ar', 0.9999999999868978],\n",
       " ['ar', 0.9999956255044523],\n",
       " ['ar', 0.99999530540245],\n",
       " ['ar', 0.9999999999999973],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999196667637],\n",
       " ['ar', 0.9999984769317508],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999980421419],\n",
       " ['ar', 0.9999975242218906],\n",
       " ['ar', 0.9999972141198092],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999985367722],\n",
       " ['ar', 0.999999973294466],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999810647],\n",
       " ['ar', 0.9999970148179984],\n",
       " ['ar', 0.9999968253077783],\n",
       " ['ar', 0.9999977409225305],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999968254274445],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999977248599782],\n",
       " ['ar', 0.9999999972902198],\n",
       " ['ar', 0.9999999999999927],\n",
       " ['ar', 0.9999999974740967],\n",
       " ['ar', 0.9999967745573788],\n",
       " ['ar', 0.8571402617343649],\n",
       " ['ar', 0.9999983503331674],\n",
       " ['ar', 0.9999962699768938],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999989925799],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999989331272274],\n",
       " ['ar', 0.9999958455694167],\n",
       " ['ar', 0.9999956141833007],\n",
       " ['ar', 0.9999986599964987],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999997708],\n",
       " ['ar', 0.9999999997861131],\n",
       " ['ar', 0.999999999990328],\n",
       " ['ar', 0.9999967951700075],\n",
       " ['ar', 0.9999965978271836],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999970103952402],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999981680368],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999989160053891],\n",
       " ['ar', 0.999999999999994],\n",
       " ['ar', 0.9999966691639206],\n",
       " ['ar', 0.9999962844530272],\n",
       " ['fa', 0.9759147289040605],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 0.9999984685894772],\n",
       " ['ar', 0.9999979007429001],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999973301822462],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999966782808727],\n",
       " ['ar', 0.9999996857505319],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999980409530891],\n",
       " ['ar', 0.9999998512580598],\n",
       " ['ar', 0.9999999999358093],\n",
       " ['ar', 0.9999998812524383],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.99999799449126],\n",
       " ['ar', 0.9999987601845941],\n",
       " ['ar', 0.9999999563728669],\n",
       " ['ar', 0.9999999571832173],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999998394358],\n",
       " ['ar', 0.9999979428365712],\n",
       " ['ar', 0.9999987233870248],\n",
       " ['fa', 0.9996678874163488],\n",
       " ['ar', 0.9999981978166093],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999967766027],\n",
       " ['ar', 0.9999999904196825],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999954309567678],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 0.9999943485228651],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ur', 0.8608002494660589],\n",
       " ['ar', 0.9999968753451297],\n",
       " ['ar', 0.9999991000400302],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999995193239247],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999851],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999963029918854],\n",
       " ['ar', 0.999999999999917],\n",
       " ['ar', 0.9999977807530963],\n",
       " ['ar', 0.9999999999996765],\n",
       " ['ar', 0.999999997051416],\n",
       " ['ar', 0.9999971477200709],\n",
       " ['ar', 0.9999980502303016],\n",
       " ['ar', 0.9999999999888805],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999977252418659],\n",
       " ['ar', 0.9999980284887666],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999989492356658],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999909764931],\n",
       " ['ar', 0.9999962651115317],\n",
       " ['ar', 0.999999957993579],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999539492964472],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999979494303],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999998995060339],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999992665617338],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999905],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999962569706962],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999999087726],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999998163795],\n",
       " ['ar', 0.9999968417918633],\n",
       " ['ar', 0.9999999999999443],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999996774572303],\n",
       " ['ar', 0.8416962801585102],\n",
       " ['ar', 0.9999998067389715],\n",
       " ['ar', 0.9585772325092972],\n",
       " ['ar', 0.9999973609091187],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999998224167],\n",
       " ['ar', 0.9999999999975357],\n",
       " ['ar', 0.8571411743084545],\n",
       " ['ar', 0.9999999999914595],\n",
       " ['ar', 0.9999977758776486],\n",
       " ['ar', 0.9999967419910228],\n",
       " ['ar', 0.9999989929778812],\n",
       " ['ar', 0.9999958245525573],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999964394843134],\n",
       " ['ar', 0.9999999999990756],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999910811],\n",
       " ['ar', 0.9999999999878257],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999985621228],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999994228381186],\n",
       " ['fa', 0.9999999998302185],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999746321],\n",
       " ['ar', 0.9999985846432189],\n",
       " ['ar', 0.9999999999998963],\n",
       " ['ar', 0.9999977761396496],\n",
       " ['ar', 0.9999978377561438],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999918927608],\n",
       " ['ar', 0.9999969187112234],\n",
       " ['ar', 0.9999999999998168],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999997682],\n",
       " ['ar', 0.9999999578131828],\n",
       " ['ar', 0.9999999999999969],\n",
       " ['ar', 0.9999999999963993],\n",
       " ['ar', 0.9999999981838357],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 0.9999999999999902],\n",
       " ['ar', 0.9999967539304911],\n",
       " ['ar', 0.9999999999999969],\n",
       " ['ar', 0.9999999999999394],\n",
       " ['ar', 0.9999997899271841],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999986240342],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999081975697],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999965763977],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999997389926],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999978],\n",
       " ['ar', 0.9999980084264439],\n",
       " ['ur', 0.9999956860651625],\n",
       " ['ar', 0.9999987968125544],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.7735879675608455],\n",
       " ['ar', 0.9999999999993225],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999469],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999998253511619],\n",
       " ['ar', 0.9999999999999591],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999986236739984],\n",
       " ['ar', 0.9999999999997318],\n",
       " ['ar', 0.9999985357026399],\n",
       " ['ar', 0.9999975352485501],\n",
       " ['fa', 0.9961006971450801],\n",
       " ['ar', 0.9999990582199765],\n",
       " ['ar', 0.9999979669559607],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999961735340246],\n",
       " ['ar', 0.9999967075762742],\n",
       " ['ar', 0.9999994728051801],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999974043],\n",
       " ['ar', 0.9999988458545125],\n",
       " ['ar', 0.9999999999999909],\n",
       " ['ar', 0.999999999992865],\n",
       " ['ar', 0.9999976926194991],\n",
       " ['ar', 0.9999972604491216],\n",
       " ['ar', 0.9999999999999964],\n",
       " ['ar', 0.9999999723213796],\n",
       " ['ar', 0.9999959379883526],\n",
       " ['ar', 0.9999999955792538],\n",
       " ['ar', 0.9999999999998668],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999998064533],\n",
       " ['ar', 0.9999999827911131],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999959662954595],\n",
       " ['ar', 0.9999945586977279],\n",
       " ['ar', 0.9999999999999811],\n",
       " ['ar', 0.999998220013782],\n",
       " ['ar', 0.999998089590217],\n",
       " ['ar', 0.9999998345211728],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.999998446924218],\n",
       " ['ar', 0.9999999489136857],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9999973195267978],\n",
       " ['ar', 0.9999999999990699],\n",
       " ['ar', 0.9999993663373234],\n",
       " ['ar', 0.9999974289838334],\n",
       " ['fa', 0.9999999760631212],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999967933539194],\n",
       " ['fa', 0.9948034978054299],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999953499],\n",
       " ['ar', 0.9999995458012548],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999985689851685],\n",
       " ['ar', 0.9999990677233748],\n",
       " ['ar', 0.9999981403767357],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999963277799],\n",
       " ['ar', 0.9999999830180525],\n",
       " ['ar', 0.9999979644481807],\n",
       " ['ar', 0.9999981165373474],\n",
       " ['ar', 0.9999999999023166],\n",
       " ['ar', 0.9999999873347684],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999981348],\n",
       " ['ar', 0.9999970150528141],\n",
       " ['ar', 0.9999981468322076],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999862945466],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999964575862056],\n",
       " ['ar', 0.9999961425718096],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999982529384911],\n",
       " ['ar', 0.999999292494415],\n",
       " ['ar', 0.9999995221221818],\n",
       " ['ar', 0.9999977192683216],\n",
       " ['ar', 0.999998059564542],\n",
       " ['ar', 0.9999999999978773],\n",
       " ['ar', 0.9999963334297137],\n",
       " ['ar', 0.99999879109091],\n",
       " ['ar', 0.9999991837986055],\n",
       " ['ar', 0.9999954927456761],\n",
       " ['ar', 0.9999980546034524],\n",
       " ['ar', 0.999999999993149],\n",
       " ['ar', 0.9999984964473212],\n",
       " ['ar', 0.9999999999999987],\n",
       " ['ar', 0.9999983832907997],\n",
       " ['ar', 0.9999999951941585],\n",
       " ['ar', 0.9999991398333876],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999991962147466],\n",
       " ['ar', 0.9999973490601506],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 0.99999999999995],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999996214751018],\n",
       " ['ar', 0.9999959552086357],\n",
       " ['ar', 0.9999999293063208],\n",
       " ['ar', 0.9999967872039108],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999996889215079],\n",
       " ['ar', 0.9999982740917344],\n",
       " ['ar', 0.9999983938812453],\n",
       " ['ar', 0.9999946750350824],\n",
       " ['ar', 0.9999971795970618],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999156846],\n",
       " ['ar', 0.9999998134212378],\n",
       " ['ar', 0.9999999974009566],\n",
       " ['ar', 0.9999990762118287],\n",
       " ['ar', 0.999998355788019],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999976145030138],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999988940238413],\n",
       " ['ar', 0.9999963421671728],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999977427367914],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999907574],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999826298261],\n",
       " ['ar', 0.9999999999999993],\n",
       " ['ar', 0.9999983806573574],\n",
       " ['ar', 0.9999999999995624],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999993],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999961804136046],\n",
       " ['ar', 0.9999974064390087],\n",
       " ['ar', 0.9999991000673613],\n",
       " ['ar', 0.9999953902563015],\n",
       " ['ar', 0.9999999999885192],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999973323814904],\n",
       " ['ar', 0.999998279439639],\n",
       " ['ar', 0.9999999999998381],\n",
       " ['ar', 0.9999996088873044],\n",
       " ['ar', 0.9999973041708861],\n",
       " ['ar', 0.9999970827390673],\n",
       " ['ar', 0.9999999922161887],\n",
       " ['ar', 0.9999976157130509],\n",
       " ['ar', 0.9999995182825807],\n",
       " ['ar', 0.9999950526902822],\n",
       " ['ar', 0.9999999999870572],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999922],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999998608254543],\n",
       " ['ar', 0.9999980903921092],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999967436962673],\n",
       " ['ar', 0.9999994615685011],\n",
       " ['ar', 0.9999995720886922],\n",
       " ['fa', 0.9995452034189314],\n",
       " ['ar', 0.999999970537373],\n",
       " ['ar', 0.999997639290763],\n",
       " ['ar', 0.999997646276834],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9972598333585708],\n",
       " ['ar', 0.9999999466174132],\n",
       " ['ar', 0.9996093502319934],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9999984932508978],\n",
       " ['ar', 0.9999962238220094],\n",
       " ['ar', 0.9999974546153949],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999976643003002],\n",
       " ['ar', 0.9999983316592621],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999992181467214],\n",
       " ['ar', 0.9999982649865067],\n",
       " ['ar', 0.9999968113356978],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999097110723453],\n",
       " ['ar', 0.9999960871772974],\n",
       " ['ar', 0.9999999999999962],\n",
       " ['ar', 0.9999999988211745],\n",
       " ['ar', 0.9999999999701481],\n",
       " ['ar', 0.999998455935375],\n",
       " ['ar', 0.9999999993753019],\n",
       " ['ar', 0.9999999979043261],\n",
       " ['ar', 0.9999991138811862],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999989611183061],\n",
       " ['ar', 0.999998018698435],\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. combined method :\n",
    "res_lang_combined = []\n",
    "\n",
    "for i in range(0, len(res_langdetect_entire)):\n",
    "    if(df1.probability[i]>df2.probability[i]):\n",
    "        res_lang_combined.append(df1.iloc[i].tolist())\n",
    "    else:\n",
    "        res_lang_combined.append(df2.iloc[i].tolist())\n",
    "res_lang_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21350.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.999318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.011593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.571426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        probability\n",
       "count  21350.000000\n",
       "mean       0.999318\n",
       "std        0.011593\n",
       "min        0.571426\n",
       "25%        1.000000\n",
       "50%        1.000000\n",
       "75%        1.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. visualise the combined method\n",
    "df3 = pd.DataFrame(res_lang_combined)\n",
    "df3.columns = ['language','probability']\n",
    "df3.describe()\n",
    "\n",
    "#4. yes. when visualising the probability discription of this combined method \n",
    "#we find (mean=0.999339, min=0.571425, 25%=1, 50%=1, 75%=1, max=1) which is more accurate than the two individual methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a dictionary-based language classifier\n",
    "- **Step 1**: Divide each corpus into a training corpus (70%) and test corpus (30%).\n",
    "- **Step 2**: learn a set of typical words (also called stop words) of **every language** (TUN and ARA) based on its training corpus.\n",
    "- **Step 3**: create a language identification algorithm that takes the list of typical words of each language and a new document as input; and returns the language of this document as output.\n",
    "- **Step 4**: Evaluate the performance of this algorithm based on the test corpus -- calculate classification accuracy, precision, recall, F1, and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 5\n",
    "Implement each step by following the instructions in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1   COMPLETE THE CODE BELOW\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#?train_test_split\n",
    "\n",
    "tun_corpus_clean_train, tun_corpus_clean_test = train_test_split(tun_corpus_clean, test_size=0.3, random_state=5 )\n",
    "ara_corpus_clean_train, ara_corpus_clean_test = train_test_split(ara_corpus_clean, test_size=0.3, random_state=5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2   Follow the instructions below\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "P = 1000   ## configuration hyperparameter; you can modify it if you want; see instructions below.\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "\n",
    "## Find typical words of the TUN language\n",
    "\n",
    "# create TfidfVectorizer instance with maxdf = 1.0 so that the most frequent words of the corpus are NOT thrown away\n",
    "bow_model_tun = TfidfVectorizer (max_df = 1.0, min_df = 0.01, use_idf = True)\n",
    "\n",
    "# call fit() method with our TUN corpus; this will create the vocabulary of the corpus ...\n",
    "bow_model_tun.fit( tun_corpus_clean_train )\n",
    "\n",
    "# select P words from this vocabulary that have the SMALLEST IDF values -- See the source code in TD1 for help ...\n",
    "tfidf_vocab=bow_model_tun.get_feature_names ()\n",
    "typical_words_tun = (pd.DataFrame(dict(Word=tfidf_vocab,IDF=bow_model_tun.idf_)).sort_values(\"IDF\", inplace=False, ascending = True)[0:P]).values[:,0].tolist()\n",
    "\n",
    "# Note: we do not need to create the DTM matrix in this part.\n",
    "\n",
    "\n",
    "## Find typical words of the ARA language: repeat same steps as above.\n",
    "bow_model_ara = TfidfVectorizer (max_df = 1.0, min_df = 0.01, use_idf = True)\n",
    "\n",
    "bow_model_ara.fit( ara_corpus_clean_train )\n",
    "\n",
    "tfidf_vocab=bow_model_ara.get_feature_names ()\n",
    "\n",
    "typical_words_ara = (pd.DataFrame(dict(Word=tfidf_vocab,IDF=bow_model_ara.idf_)).sort_values(\"IDF\", inplace=False, ascending = True)[0:P]).values[:,0].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 -- write the algorithm for dictionary-based language identification. \n",
    "#    This algorithm selects the language that has the highest number of typical words in the input document.\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "def dict_langid(typical_words,doc):\n",
    "    i=0\n",
    "    for word in doc.split():\n",
    "        if word in typical_words :\n",
    "            i+=1;\n",
    "    frac=i/len(doc)\n",
    "    return frac\n",
    "\n",
    "\n",
    "\n",
    "# for each document in the test combined test corpus, call dict_langid with typical_words_tun and then with typical_words_tun\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "for doc in tun_corpus_clean_test:\n",
    "    frac_tun=dict_langid(typical_words_tun, doc)\n",
    "    frac_ara=dict_langid(typical_words_ara, doc)\n",
    "    if(frac_tun>=frac_ara):\n",
    "        y_pred.append(\"tun\")\n",
    "    else:\n",
    "        y_pred.append(\"ara\")\n",
    "    y_true.append(\"tun\")\n",
    "\n",
    "for doc in ara_corpus_clean_test:\n",
    "    frac_tun=dict_langid(typical_words_tun, doc)\n",
    "    frac_ara=dict_langid(typical_words_ara, doc)\n",
    "    if(frac_ara>=frac_tun):\n",
    "        y_pred.append(\"ara\")\n",
    "    else:\n",
    "        y_pred.append(\"tun\")\n",
    "    y_true.append(\"ara\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9261015609563328"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4\n",
    "# Reference: see scikit-learn documentation\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"Accuracy score : \")\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6366,   39],\n",
       "       [ 709, 3008]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Confusion matrix : \")\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.916962466680822"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"f1 score : \")\n",
    "f1_score(y_true, y_pred, average='macro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9434942554861934"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"precision score : \")\n",
    "precision_score(y_true, y_pred, average='macro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall score : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9015828911911157"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"recall score : \")\n",
    "recall_score(y_true, y_pred, average='macro')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a language classifier using supervised learning\n",
    "- **Step 0**: Divide each corpus into a training corpus (70%) and test corpus (30%).\n",
    "- **Step 1**: Create a data frame called ``train_df`` that has two columns: 'document' and 'language'. The 'document' column contains the two corpora concatenated together. The values in the '' column should be 'TUN' and 'ARA'.  Repeat the same thing for the ``test_df``.\n",
    "- **Step 2**: Convert the training documents into numeric feature vectors using the BOW-tfidf method with **character ngrams**.\n",
    "- **Step 3**: Create a language classifier using Naive Bayes method (tfidf version).\n",
    "- **Step 4**: Evaluate performance of this classifier based on the test corpus -- calculate classification accuracy, precision, recall, F1, and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 6\n",
    "Implement each step by carefully following the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 just use the same variables that you created in Step 1 of the previous part.\n",
    "# Nothing to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مسخة خامجة تحسايب روحها مزيانة من توا تفيش علي...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ربي معاك و انشاء الله ربي يعاونك</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>شمس تحرقكم ولا تربحكم ونشالله تتشوي فها الشمس</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>الليلة شيخة ضحك مع جعفور محلاه ياسر نحبو كان ف...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>برا مخيب وجهك</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document language\n",
       "0  مسخة خامجة تحسايب روحها مزيانة من توا تفيش علي...      TUN\n",
       "1                   ربي معاك و انشاء الله ربي يعاونك      TUN\n",
       "2      شمس تحرقكم ولا تربحكم ونشالله تتشوي فها الشمس      TUN\n",
       "3  الليلة شيخة ضحك مع جعفور محلاه ياسر نحبو كان ف...      TUN\n",
       "4                                      برا مخيب وجهك      TUN"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1   create 2 data frames called train_df and test_df (as explained above)\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "# create data frame\n",
    "train_df = pd.DataFrame({'document':[], 'language':[]})\n",
    "\n",
    "# fill the language column\n",
    "train_df.language = pd.Series(['TUN']*len(tun_corpus_clean_train) + ['ARA']*len(ara_corpus_clean_train))\n",
    "\n",
    "\n",
    "# fill the document column -- CONCATENATE the TUN CORPUS and ARA CORPUS\n",
    "train_df.document = pd.Series(tun_corpus_clean_train + ara_corpus_clean_train)\n",
    "\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>هذه الانسانة ضاربها الفشل من راسها للساقيها اذ...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>انت معلم اما رد بالك تلبس كيما في لسه فاكر</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>برافوو للاخت سميرة التي وقفت بجانب اخيها في مح...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>مح لا ك منو لا امزينيك</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>برافو فوزي</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document language\n",
       "0  هذه الانسانة ضاربها الفشل من راسها للساقيها اذ...      TUN\n",
       "1         انت معلم اما رد بالك تلبس كيما في لسه فاكر      TUN\n",
       "2  برافوو للاخت سميرة التي وقفت بجانب اخيها في مح...      TUN\n",
       "3                             مح لا ك منو لا امزينيك      TUN\n",
       "4                                         برافو فوزي      TUN"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create data frame\n",
    "test_df = pd.DataFrame({'document':[], 'language':[]})\n",
    "\n",
    "# fill the language column\n",
    "test_df.language = pd.Series(['TUN']*len(tun_corpus_clean_test) + ['ARA']*len(ara_corpus_clean_test))\n",
    "\n",
    "\n",
    "# fill the document column -- CONCATENATE the TUN CORPUS and ARA CORPUS\n",
    "test_df.document = pd.Series(tun_corpus_clean_test + ara_corpus_clean_test)\n",
    "\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the number of rows in this data frame = sum of the number of documents in each corpus.  \n",
    "assert(train_df.shape[0] == len(tun_corpus_clean_train) + len(ara_corpus_clean_train))\n",
    "assert(train_df.shape[1] == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(train_df.language.nunique() == 2 and train_df.language.unique().tolist() == ['TUN', 'ARA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TUN', 'ARA'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2  Convert the training documents into numeric feature vectors using the BOW-tfidf method with character ngrams.\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "n = 3   # hyperparameter for of character ngrams ; you can change it if you want but n=3 is a reaonable value ...\n",
    "\n",
    "# Create an instance of TfidfVectorizer class with analyzer = 'char' so that it generates bag of characters and not bag of words\n",
    "bow_model_char = TfidfVectorizer(analyzer='char', ngram_range=(1,n), max_df = 0.9, min_df = 0.2)\n",
    "\n",
    "# Call fit method with the combined training corpus\n",
    "bow_model_char.fit(train_df.document)\n",
    "\n",
    "# Create DTM matrix of the combined training corpus and test corpus\n",
    "train_dtm = bow_model_char.transform(train_df.document)\n",
    "test_dtm = bow_model_char.transform(test_df.document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3   -- see official documentation of MultinomialNB in scikit-learn\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "nb_model.fit(train_dtm,train_df.language)\n",
    "\n",
    "y_pred_class = nb_model.predict(test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ARA', 'TUN', 'ARA', ..., 'ARA', 'ARA', 'ARA'], dtype='<U3')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.897549891325825"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4   Use the same source code as in Step 4 of the previous part.\n",
    "# accuracy, precision, recall, F1, and confusion matrix\n",
    "print(\"Accuracy score : \")\n",
    "accuracy_score(test_df.language, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9056652846495349"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"precision score : \")\n",
    "precision_score(test_df.language, y_pred_class, average=\"macro\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall score : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8734335375346767"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"recall score : \")\n",
    "recall_score(test_df.language, y_pred_class, average=\"macro\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8856359600200889"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"f1 score : \")\n",
    "f1_score(test_df.language, y_pred_class, average='macro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6176,  229],\n",
       "       [ 808, 2909]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Confusion matrix : \")\n",
    "confusion_matrix(test_df.language, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
