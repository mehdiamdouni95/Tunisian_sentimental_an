{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline:**\n",
    "1. Read raw data\n",
    "2. Clean data\n",
    "3. Try existing language identification libraries\n",
    "4. Construct our own dictionary-based language classifier\n",
    "5. Construct our own language classifier using supervised learning\n",
    "\n",
    "**What you need to do:** \n",
    "- Read and execute the source code below and answer the questions in **EXERCISE 1 - EXERCISE 6**. \n",
    "- **Submit** the modifiled file ``TD2.ipynb`` on google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set the font size of plots\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_files = ['./langid_data_TUN-AR.txt', './langid_data_ARA.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    print('Reading file ' + filename + \"...\")\n",
    "    with open(filename, \"r\", encoding='utf8') as textfile:\n",
    "        L = []\n",
    "        for line in textfile:\n",
    "            L.append(line.strip())\n",
    "        print('File contains ', len(L), \"lines.\\n\")\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ./langid_data_TUN-AR.txt...\n",
      "File contains  13932 lines.\n",
      "\n",
      "Reading file ./langid_data_ARA.txt...\n",
      "File contains  21787 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tun_corpus = read_text_file(corpus_files[0])\n",
    "ara_corpus = read_text_file(corpus_files[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "For language identification, we usually only need to remove non-word characters and replace them with spaces. But since our corpus contains social media text, a few other operations are necessary.\n",
    "\n",
    "- Remove non-word symbols (punctuation, math symbols, emoticons, URLs, hashtags, etc.).\n",
    "- Replace punctuation and white space with a single space.\n",
    "- Normalize word elongations and word repetitions.\n",
    "- Remove documents that contain a large fraction of latin characters (some documents contain english or french words).\n",
    "- Remove very short documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This just a simple js jshd jhsd js dh asjh test'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# regexp for word elongation: matches 3 or more repetitions of a word character.\n",
    "two_plus_letters_RE = re.compile(r\"(\\w)\\1{1,}\", re.DOTALL)\n",
    "three_plus_letters_RE = re.compile(r\"(\\w)\\1{2,}\", re.DOTALL)\n",
    "# regexp for repeated words\n",
    "two_plus_words_RE = re.compile(r\"(\\w+\\s+)\\1{1,}\", re.DOTALL)\n",
    "\n",
    "\n",
    "def cleanup_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "\n",
    "    # Remove user mentions of the form @username\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "    \n",
    "    # Replace special html-encoded characters with their ASCII equivalent, for example: &#39 ==> '\n",
    "    #if re.search(\"&#\",text):\n",
    "        #text = html.unescape(text)\n",
    "\n",
    "    # Remove special useless characters such as _x000D_\n",
    "    text = re.sub(r'_[xX]000[dD]_', '', text)\n",
    "\n",
    "    # Replace all non-word characters (such as emoticons, punctuation, end of line characters, etc.) with a space\n",
    "    text = re.sub('[\\W_]', ' ', text)\n",
    "\n",
    "    # Remove redundant white spaces\n",
    "    text = text.strip()\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "\n",
    "    # normalize word elongations (characters repeated more than twice)\n",
    "    text = two_plus_letters_RE.sub(r\"\\1\\1\", text)\n",
    "\n",
    "    # remove repeated words\n",
    "    text = two_plus_words_RE.sub(r\"\\1\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# unit test of this function\n",
    "cleanup_text(\"This is just a simple. js .... jshd)jhsd__js--dh \\n\\n asjh\\n test !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 1\n",
    "\n",
    "Do the following operations for **each** corpus. Store the new corpora in new variables called ``tun_corpus_clean`` and ``ara_corpus_clean``\n",
    "1. Clean up each document in the corpus using the ``cleanup_text`` function given above.\n",
    "2. Remove all documents that contain a large fraction of latin characters (for example more than 70%).  **Hint**: use a regular expression with pattern [a-zA-Z]\n",
    "3. Remove very short documents (containing less than 10 characters, for e.g.).\n",
    "4. Display the number of documents in the clean corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13932 21787\n"
     ]
    }
   ],
   "source": [
    "## COMPLETE THE CODE BELOW\n",
    "#1\n",
    "tun_corpus_clean = [cleanup_text(doc) for doc in tun_corpus]\n",
    "ara_corpus_clean = [cleanup_text(doc) for doc in ara_corpus]\n",
    "\n",
    "print(len(tun_corpus_clean), len(ara_corpus_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "#tun_corpus_clean=[re.sub('[a-zA-Z]+','',doc) for doc in tun_corpus_clean]\n",
    "tun_corpus_clean2=[doc for doc in tun_corpus_clean if (len(re.findall('[a-zA-Z]+',doc))/len(doc))<0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "#ara_corpus_clean=[re.sub(' [a-zA-Z]+','',doc) for doc in ara_corpus_clean]\n",
    "ara_corpus_clean2=[doc for doc in ara_corpus_clean if (len(re.findall('[a-zA-Z]+',doc))/len(doc))<0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12039 21459\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "tun_corpus_clean3 = [cleanup_text(doc) for doc in tun_corpus_clean2 if len(doc)>10]\n",
    "ara_corpus_clean3 = [cleanup_text(doc) for doc in ara_corpus_clean2 if len(doc)>10]\n",
    "print(len(tun_corpus_clean3), len(ara_corpus_clean3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try existing language identification libraries\n",
    "- In TD1, we used NLTK's textcat library.\n",
    "- In this TD, we will use another 2 libraries (``langdetect`` and ``langid``) which are actually more accurate than ``textcat``. However, as you will see they "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load special libraries for language identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = \"This is just a simple test !!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[en:0.9999979865821665]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# langdetect library\n",
    "\n",
    "import langdetect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "try:\n",
    "    res = langdetect.detect_langs(test_doc)   # LANGDETECT\n",
    "    #res = langdetect.detect(test_doc) \n",
    "except LangDetectException:\n",
    "    res = langdetect.language.Language(\"UNK\",0)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('en', 0.9999998819046767)\n",
      "[('en', 0.9999998819046767), ('br', 1.1536448337940777e-07), ('la', 1.4557736338637932e-09)]\n"
     ]
    }
   ],
   "source": [
    "# langid library\n",
    "\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "li = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "print(li.classify(test_doc))\n",
    "print(li.rank(test_doc)[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eng'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK textcat library - JUST IN CASE\n",
    "from nltk.classify.textcat import TextCat\n",
    "\n",
    "# create class instance\n",
    "tc = TextCat()\n",
    "tc.guess_language(test_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ``langdetect`` on a small sample of Tunisian Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a small random sample of 5000 documents from the corpus\n",
    "n = 5000\n",
    "random_indices = np.random.choice(np.arange(len(tun_corpus_clean3)), n, replace=False)\n",
    "small_corpus = [tun_corpus_clean[i] for i in random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_langdetect = []\n",
    "\n",
    "for doc in small_corpus:\n",
    "    try:\n",
    "        res_langdetect.append(langdetect.detect_langs(doc))\n",
    "    except LangDetectException:\n",
    "        res_langdetect.append([langdetect.language.Language(\"UNK\",0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_corpus),len(res_langdetect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>language</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مله مناظرامقلبه جابتهم هاثوره الكلبه تفوه</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وحدك لطفي</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>موزاييك اذاعة الفواحش اصلا هه</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>شكونك انتي كنقارنوك باخيب امرة اكرانية من ناحي...</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>خير من نوفل</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document language  probability\n",
       "0          مله مناظرامقلبه جابتهم هاثوره الكلبه تفوه       ar     0.999997\n",
       "1                                          وحدك لطفي       ar     0.999999\n",
       "2                      موزاييك اذاعة الفواحش اصلا هه       ar     0.999999\n",
       "3  شكونك انتي كنقارنوك باخيب امرة اكرانية من ناحي...       ar     0.999998\n",
       "4                                        خير من نوفل       ar     0.999997"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's put the results in a data frame for ease of manipulation\n",
    "\n",
    "def foo1(x):\n",
    "    u = str(x).split(':')\n",
    "    return [u[0],float(u[1])]\n",
    "L = [[small_corpus[i]]+foo1(x[0]) for i,x in enumerate(res_langdetect)]\n",
    "df = pd.DataFrame(L)\n",
    "df.columns = ['document', 'language','probability']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 3\n",
    "1. Plot the distribution of languages (call value_counts() on the ``language`` column of ``df``)\n",
    "2. Plot the histogram of probabilities (call plot(kind='hist') on the ``probability`` column of ``df``)\n",
    "3. What are the top 2 languages identified in this corpus?\n",
    "4. For what fraction of documents is this language identifier more than 80% confident in its decision? (i.e. probability > 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENTER YOUR ANSWERS BELOW\n",
    "\n",
    "...\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18b8c4ed5c0>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ENTER YOUR SOURCE CODE BELOW\n",
    "\n",
    "#1\n",
    "plt.plot(df.language.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18b8c4e1940>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2\n",
    "df.probability.plot(kind='hist')\n",
    "#3\n",
    "# the top 2 languages identified in this corpus are Arabe and farsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9746"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4\n",
    "fraction=len([a for a in df.probability if a >0.8]) / len(df.probability)\n",
    "fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 4 (homework - skip this during class)\n",
    "\n",
    "1. Test both libraries (``langdetect`` and ``langid``) on the **entire** ``ara_corpus_clean`` corpus.\n",
    "2. Which library seems to be more accurate for the Arabic language? Justify based on visual inspection of the results.\n",
    "3. Design a new language identification method that combines both libraries -- basically just use the result corresponding to the higher probability.\n",
    "4. Is this new method more accurate than the individual methods? Again, justify your answer based on visual inspection of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responses:\n",
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a small random sample of 5000 documents from the corpus\n",
    "n = 5000\n",
    "random_indices = np.random.choice(np.arange(len(ara_corpus_clean3)), n, replace=False)\n",
    "small_corpus_ara = [ara_corpus_clean[i] for i in random_indices]\n",
    "\n",
    "#langid\n",
    "res_langid_ara = []\n",
    "for doc in small_corpus_ara:\n",
    "    res_langid_ara.append(li.classify(doc))\n",
    "    \n",
    "#langdetect\n",
    "res_langdetect_ara = []\n",
    "\n",
    "for doc in small_corpus_ara:\n",
    "    try:\n",
    "        res_langdetect_ara.append(langdetect.detect_langs(doc))\n",
    "    except LangDetectException:\n",
    "        res_langdetect_ara.append([langdetect.language.Language(\"UNK\",0)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. According to the following results,Langdetect seems to be more accurate for the Arabic language because it has the highest mean. (0.99>0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.996687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.033840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       probability\n",
       "count  5000.000000\n",
       "mean      0.996687\n",
       "std       0.033840\n",
       "min       0.000000\n",
       "25%       0.999997\n",
       "50%       0.999998\n",
       "75%       0.999999\n",
       "max       1.000000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L=[foo1(x[0]) for i,x in enumerate(res_langdetect_ara)]\n",
    "df = pd.DataFrame(L)\n",
    "df.columns = ['language','probability']\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.989119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.063323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.169462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       probability\n",
       "count  5000.000000\n",
       "mean      0.989119\n",
       "std       0.063323\n",
       "min       0.169462\n",
       "25%       1.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfid = pd.DataFrame(res_langid_ara)\n",
    "dfid.columns = ['language', 'probability']\n",
    "dfid.head()\n",
    "dfid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The new language identification method that combines both libraries is : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ar', 0.9999982480653715],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999997533],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999964200235134],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999998681559],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999991986095],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999992943],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999998291804134],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999989153293266],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999991988257696],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999916325598],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999996],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999996],\n",
       " ['ar', 0.9999970581110003],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999929],\n",
       " ['ar', 0.9999999999999765],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999893449384],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999976673048858],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999989],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999998054088],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999991],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999973014979634],\n",
       " ['ar', 0.9999967879181783],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999998026288],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999967728616628],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999989],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999931363364],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999986343483855],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999484852669],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999979317487229],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.7142839052052276],\n",
       " ['ar', 0.999998358003115],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999996756],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.8571423365291533],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ur', 0.9999959669324426],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999953985532402],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999944888964],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999975962359011],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999955461416649],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999441019807],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.8571397860640332],\n",
       " ['ar', 0.9999993265741095],\n",
       " ['ar', 0.9999999182477803],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999991196013815],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999964587514861],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999985581833892],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999982121114007],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999999778924],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999910454],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999994114201711],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999427673989],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999967042272285],\n",
       " ['ar', 0.9999963892992476],\n",
       " ['zh', 1.0],\n",
       " ['ar', 0.9999974751144731],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999995617823],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999971008926138],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999980060124491],\n",
       " ['ar', 1.0],\n",
       " ['ps', 0.7735749992083818],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999997891823],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999998894],\n",
       " ['ar', 0.9999988479780341],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999139973398471],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999976531479611],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999979546519425],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999797059995],\n",
       " ['ar', 0.9999999999999192],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999984761212],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999995595],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999953701],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999999998914],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999974003775073],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999977297375794],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999988946794753],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999996807804048],\n",
       " ['ar', 0.9999978254180324],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999971774],\n",
       " ['ar', 0.9999999998315017],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9307376208108429],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999997187],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999979786842036],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999980069635593],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999054812994],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999993233586204],\n",
       " ['ar', 0.9999999803274783],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999972607362405],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999984983090693],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999955740438607],\n",
       " ['ar', 0.9999980890300383],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999984],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999976186],\n",
       " ['ar', 0.9999999999676692],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999985621228],\n",
       " ['ar', 0.9999985501285619],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999978349583715],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999984],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9999989590672005],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999997226490568],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999996286268],\n",
       " ['ar', 0.9999999999974625],\n",
       " ['ar', 0.9999999995999702],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999987],\n",
       " ['ar', 0.999999998012715],\n",
       " ['ar', 1.0],\n",
       " ['en', 0.9999963753119117],\n",
       " ['ar', 0.9999999875943036],\n",
       " ['ar', 0.9999999995444222],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999986943450858],\n",
       " ['ar', 0.9999998492795684],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999956],\n",
       " ['en', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999974351059703],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999963462656112],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999944216672523],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999993581847],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999963831874041],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999995997],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999973],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999985858309061],\n",
       " ['ar', 0.9999999999952092],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999969],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999991818],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999984],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999988028721748],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9999999999521523],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999998253],\n",
       " ['ar', 0.9999997632053282],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999994125],\n",
       " ['ar', 0.9999999999995657],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999989765996],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999969837496754],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999997906],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999988967647703],\n",
       " ['ar', 1.0],\n",
       " ['en', 0.9955163413692784],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999966085054706],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999695182],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999992663133248],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999974275756647],\n",
       " ['so', 0.9999983624504234],\n",
       " ['ar', 0.9999979527800309],\n",
       " ['ar', 0.9999973316233092],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999993],\n",
       " ['ar', 0.9999941635909935],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999997691045],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999970320841493],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999656830897],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999981563623465],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999984],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999993037239],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.8571403109009033],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999964329730323],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999996338068353],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999411213],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999991405663732],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999992553026],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999982217152927],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999963258501993],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999687],\n",
       " ['ar', 0.9999999979358409],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999998887996],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999997538418],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999989929778812],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999904599549],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999969251189909],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999986271310153],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999630278],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999992825029058],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999751],\n",
       " ['ar', 0.9999976782521712],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999538],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999649],\n",
       " ['ar', 0.9999974286869011],\n",
       " ['ar', 0.9999999999999865],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999983368901866],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999943221916923],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9999958986764991],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999997174468326],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999997977402],\n",
       " ['ar', 0.9999999864146059],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999131496313],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999973878250976],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999984626769711],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999999999994],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999826219],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999969791385556],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999989142693917],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999993],\n",
       " ['ar', 0.9999999906721947],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9999999658846022],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999991876912679],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.7323689071419774],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999966969736761],\n",
       " ['ar', 0.9471239060024315],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999976201395304],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999997718109362],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999993440911916],\n",
       " ['ar', 0.9999999986590962],\n",
       " ['ar', 0.9999969577057272],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9905646117237656],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999997487958517],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999975277468676],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999045],\n",
       " ['ar', 0.9999999998722431],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999981986],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9999986545860622],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999983495],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999967374608755],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999985616659419],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999750235],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999998583],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999997335589],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999988033301085],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999973528],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999986798078284],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999032485],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9999894848643572],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999997432771],\n",
       " ['ar', 0.9999968716949357],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999994456614621],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999997791],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999979351560104],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['en', 0.9999999999860438],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999995376592387],\n",
       " ['ar', 0.9999999987454051],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999995947402],\n",
       " ['ar', 0.9999973268417927],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999973577203456],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999967],\n",
       " ['ar', 0.9999977635562562],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999995479052],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999989113],\n",
       " ['ar', 0.9999999998018871],\n",
       " ['ar', 0.9999972562767178],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.999999999998374],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999960640964],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999953599697422],\n",
       " ['ar', 0.9999977595246263],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999303819],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999998054308027],\n",
       " ['ur', 0.999996904908961],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999983151],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999992496351],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999982426811919],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.999997702932793],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999847864],\n",
       " ['ar', 0.9999999999999893],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999984],\n",
       " ['ar', 0.999999999999835],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9993637685521122],\n",
       " ['ar', 0.8571397978642441],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999996],\n",
       " ['ar', 0.9999999999999996],\n",
       " ['ar', 0.9999957712772412],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999673848894],\n",
       " ['ar', 0.9999999190008925],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999970172317741],\n",
       " ['ar', 0.9999999999590694],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999991438],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ur', 0.9999968110739934],\n",
       " ['ar', 0.9999999999999849],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999966326439114],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999999999998],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999973520106915],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999977037357773],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999999995213233],\n",
       " ['ar', 0.9946647691133866],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['fa', 0.9911079081744665],\n",
       " ['fa', 0.9999998896998843],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999989481090803],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999969005305018],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ['ar', 0.9999979808058215],\n",
       " ['ar', 1.0],\n",
       " ['ar', 1.0],\n",
       " ...]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine=[]\n",
    "for i in range(0, len(res_langdetect_ara)):\n",
    "    if(df.probability[i]>dfid.probability[i]):\n",
    "        combine.append(df.iloc[i].tolist())\n",
    "    \n",
    "    else:\n",
    "        combine.append(dfid.iloc[i].tolist())\n",
    "combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.998548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.020655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       probability\n",
       "count  5000.000000\n",
       "mean      0.998548\n",
       "std       0.020655\n",
       "min       0.428571\n",
       "25%       1.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df = pd.DataFrame(combine)\n",
    "combine_df.columns = ['language','probability']\n",
    "combine_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Yes, this new method is more accurate than the individual methods because it has the highest mean (0.998>0.996>0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a dictionary-based language classifier\n",
    "- **Step 1**: Divide each corpus into a training corpus (70%) and test corpus (30%).\n",
    "- **Step 2**: learn a set of typical words (also called stop words) of **every language** (TUN and ARA) based on its training corpus.\n",
    "- **Step 3**: create a language identification algorithm that takes the list of typical words of each language and a new document as input; and returns the language of this document as output.\n",
    "- **Step 4**: Evaluate the performance of this algorithm based on the test corpus -- calculate classification accuracy, precision, recall, F1, and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 5\n",
    "Implement each step by following the instructions in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1   COMPLETE THE CODE BELOW\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#?train_test_split\n",
    "\n",
    "tun_corpus_clean_train, tun_corpus_clean_test = train_test_split(tun_corpus_clean3, test_size=0.3 )\n",
    "\n",
    "ara_corpus_clean_train, ara_corpus_clean_test = train_test_split(ara_corpus_clean3, test_size=0.3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>في</td>\n",
       "      <td>2.503484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>من</td>\n",
       "      <td>2.896487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>يا</td>\n",
       "      <td>2.991798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>علي</td>\n",
       "      <td>3.093301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>الله</td>\n",
       "      <td>3.285877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word       IDF\n",
       "150    في  2.503484\n",
       "202    من  2.896487\n",
       "236    يا  2.991798\n",
       "132   علي  3.093301\n",
       "31   الله  3.285877"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2   Follow the instructions below\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "P = 1000   ## configuration hyperparameter; you can modify it if you want; see instructions below.\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "\n",
    "## Find typical words of the TUN language\n",
    "\n",
    "# create TfidfVectorizer instance with maxdf = 1.0 so that the most frequent words of the corpus are NOT thrown away\n",
    "bow_model_tun = TfidfVectorizer (max_df = 1.0, min_df = 0.005)\n",
    "\n",
    "# call fit() method with our TUN corpus; this will create the vocabulary of the corpus ...\n",
    "bow_model_tun.fit( tun_corpus_clean_train )\n",
    "\n",
    "# select P words from this vocabulary that have the SMALLEST IDF values -- See the source code in TD1 for help ...\n",
    "tun_vocab=bow_model_tun.get_feature_names ()\n",
    "u=pd.DataFrame(dict(Word=tun_vocab,IDF=bow_model_tun.idf_)).sort_values(\"IDF\", inplace=False, ascending = True)\n",
    "\n",
    "# Note: we do not need to create the DTM matrix in this part.\n",
    "\n",
    "typical_words_tun = u.Word[0:P]\n",
    "\n",
    "u.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>من</td>\n",
       "      <td>1.476066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>في</td>\n",
       "      <td>1.636344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>على</td>\n",
       "      <td>1.973269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>جدا</td>\n",
       "      <td>2.046537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>الفندق</td>\n",
       "      <td>2.130331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word       IDF\n",
       "1262      من  1.476066\n",
       "999       في  1.636344\n",
       "940      على  1.973269\n",
       "692      جدا  2.046537\n",
       "388   الفندق  2.130331"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_model_ara = TfidfVectorizer (max_df = 1.0, min_df = 0.005)\n",
    "\n",
    "# call fit() method with our TUN corpus; this will create the vocabulary of the corpus ...\n",
    "bow_model_ara.fit( ara_corpus_clean_train )\n",
    "\n",
    "# select P words from this vocabulary that have the SMALLEST IDF values -- See the source code in TD1 for help ...\n",
    "ara_vocab=bow_model_ara.get_feature_names ()\n",
    "v=pd.DataFrame(dict(Word=ara_vocab,IDF=bow_model_ara.idf_)).sort_values(\"IDF\", inplace=False, ascending = True)\n",
    "\n",
    "# Note: we do not need to create the DTM matrix in this part.\n",
    "\n",
    "typical_words_ara = v.Word[0:P]\n",
    "\n",
    "v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TUN\n"
     ]
    }
   ],
   "source": [
    "# Step 3 -- write the algorithm for dictionary-based language identification. \n",
    "#    This algorithm selects the language that has the highest number of typical words in the input document.\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "def dict_langid(typical_words,doc):\n",
    "    sum=0\n",
    "    for i in typical_words:\n",
    "         if i in doc:\n",
    "                sum=sum+1\n",
    "    lang=float(sum/len(doc))\n",
    "    return lang\n",
    "\n",
    "# for each document in the test combined test corpus, call dict_langid with typical_words_tun and then with typical_words_tun\n",
    "# dict_langid(typical_words_tun, doc)\n",
    "# dict_langid(typical_words_ara, doc)\n",
    "# ...\n",
    "sumARA=0\n",
    "sumTUN=0\n",
    "for doc in tun_corpus_clean_test:\n",
    "    if (dict_langid(typical_words_ara,doc) > dict_langid(typical_words_tun,doc)):\n",
    "        sumARA = sumARA+1\n",
    "    else:\n",
    "        sumTUN = sumTUN+1\n",
    "if(sumARA >= sumTUN):\n",
    "    Lang='ARA'\n",
    "else:\n",
    "    Lang='TUN'\n",
    "print(Lang)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARA\n"
     ]
    }
   ],
   "source": [
    "sumARA=0\n",
    "sumTUN=0\n",
    "for doc in ara_corpus_clean_test:\n",
    "    if (dict_langid(typical_words_ara,doc) > dict_langid(typical_words_tun,doc)):\n",
    "        sumARA = sumARA+1\n",
    "    else:\n",
    "        sumTUN = sumTUN+1\n",
    "if(sumARA >= sumTUN):\n",
    "    Lang='ARA'\n",
    "else:\n",
    "    Lang='TUN'\n",
    "print(Lang)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "# Reference: see scikit-learn documentation\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a language classifier using supervised learning\n",
    "- **Step 0**: Divide each corpus into a training corpus (70%) and test corpus (30%).\n",
    "- **Step 1**: Create a data frame called ``train_df`` that has two columns: 'document' and 'language'. The 'document' column contains the two corpora concatenated together. The values in the '' column should be 'TUN' and 'ARA'.  Repeat the same thing for the ``test_df``.\n",
    "- **Step 2**: Convert the training documents into numeric feature vectors using the BOW-tfidf method with **character ngrams**.\n",
    "- **Step 3**: Create a language classifier using Naive Bayes method (tfidf version).\n",
    "- **Step 4**: Evaluate performance of this classifier based on the test corpus -- calculate classification accuracy, precision, recall, F1, and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 6\n",
    "Implement each step by carefully following the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 just use the same variables that you created in Step 1 of the previous part.\n",
    "# Nothing to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بربي لا عاد تجيبولنا الوجوه هذي الشعب يولي يجر...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>كانت قصة جد مءثرة تحياتنا من الجزاءر</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>التافه كريم الغربي</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>انا فديت من تفاهتكم و فسادكم و بيع دينكم و بلد...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>سيد الرءيس ربي يكون معاك وربي يعينك علي كل متر...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document language\n",
       "0  بربي لا عاد تجيبولنا الوجوه هذي الشعب يولي يجر...      TUN\n",
       "1               كانت قصة جد مءثرة تحياتنا من الجزاءر      TUN\n",
       "2                                 التافه كريم الغربي      TUN\n",
       "3  انا فديت من تفاهتكم و فسادكم و بيع دينكم و بلد...      TUN\n",
       "4  سيد الرءيس ربي يكون معاك وربي يعينك علي كل متر...      TUN"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1   create 2 data frames called train_df and test_df (as explained above)\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "# create data frame\n",
    "train_df = pd.DataFrame({'document':[], 'language':[]})\n",
    "\n",
    "# fill the language column\n",
    "train_df.language = pd.Series(['TUN']*len(tun_corpus_clean_train) + ['ARA']*len(tun_corpus_clean_train))\n",
    "\n",
    "\n",
    "# fill the document column -- CONCATENATE the TUN CORPUS and ARA CORPUS\n",
    "train_df.document = pd.Series(tun_corpus_clean_train + ara_corpus_clean_train)\n",
    "\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الفة بن رمصان فنانة و نصف ابدعت</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>كوميدي رقم واحد في تونس و الشيء من ماتاه لا يس...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blablabla باين فيها سهرة مارقة نشوف التاسعة اش...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>قداش عندو احساس تبارك الله عليه j aime</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>من افضل البرامج</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document language\n",
       "0                    الفة بن رمصان فنانة و نصف ابدعت      TUN\n",
       "1  كوميدي رقم واحد في تونس و الشيء من ماتاه لا يس...      TUN\n",
       "2  blablabla باين فيها سهرة مارقة نشوف التاسعة اش...      TUN\n",
       "3             قداش عندو احساس تبارك الله عليه j aime      TUN\n",
       "4                                    من افضل البرامج      TUN"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame({'document':[], 'language':[]})\n",
    "test_df.language = pd.Series(['TUN']*len(tun_corpus_clean_test) + ['ARA']*len(tun_corpus_clean_test))\n",
    "test_df.document = pd.Series(tun_corpus_clean_test + ara_corpus_clean_test)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred\n"
     ]
    }
   ],
   "source": [
    "# verify that the number of rows in this data frame = sum of the number of documents in each corpus.  \n",
    "try:\n",
    "    \n",
    "    assert train_df.shape[0] == len(tun_corpus_clean_train) + len(ara_corpus_clean_train) \n",
    "    assert train_df.shape[1] == 2\n",
    "except AssertionError:\n",
    "    print('An error occurred')\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-b8d6fe98dfb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'TUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ARA'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "assert(train_df.language.nunique() == 2 and train_df.language.unique() == ['TUN', 'ARA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2  Convert the training documents into numeric feature vectors using the BOW-tfidf method with character ngrams.\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "\n",
    "n = 3   # hyperparameter for of character ngrams ; you can change it if you want but n=3 is a reaonable value ...\n",
    "\n",
    "# Create an instance of TfidfVectorizer class with analyzer = 'char' so that it generates bag of characters and not bag of words\n",
    "bow_model_char = TfidfVectorizer(analyzer='char', ngram_range=(1,n), max_df =0.9, min_df=0.1)\n",
    "\n",
    "# Call fit method with the combined training corpus\n",
    "bow_model_char.fit(train_df.document)\n",
    "\n",
    "# Create DTM matrix of the combined training corpus and test corpus\n",
    "dtm_Train=bow_model_char.transform(train_df.document)\n",
    "dtm_Test=bow_model_char.transform(test_df.document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ARA', 'TUN', 'TUN', ..., 'ARA', 'ARA', 'ARA'], dtype='<U3')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3   -- see official documentation of MultinomialNB in scikit-learn\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "nb_model.fit(dtm_Train,train_df.language)\n",
    "\n",
    "nb_model.predict(dtm_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9408914728682171"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4   Use the same source code as in Step 4 of the previous part.\n",
    "print(\"The accuracy score : \")\n",
    "accuracy_score(test_df.language,nb_model.predict(dtm_Test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.941025641025641"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The precision score :\")\n",
    "precision_score(test_df.language,nb_model.predict(dtm_Test), average=\"macro\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9408914728682171"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The recall: \")\n",
    "recall_score(test_df.language,nb_model.predict(dtm_Test), average=\"macro\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9408869770494759"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The f1: \")\n",
    "f1_score(test_df.language,nb_model.predict(dtm_Test), average='macro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3367,  245],\n",
       "       [ 182, 3430]], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The confusion matrix : \")\n",
    "confusion_matrix(test_df.language,nb_model.predict(dtm_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lang_id_model.pkl','wb') as f :\n",
    "    pickle.dump(nb_model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bow_model_char.pkl','wb') as f :\n",
    "    pickle.dump(bow_model_char,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TUN', 'ARA'], dtype='<U3')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiou=pd.Series(['كوميدي  ','ahla'])\n",
    "dtm=bow_model_char.transform(wiou)\n",
    "nb_model.predict(dtm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
